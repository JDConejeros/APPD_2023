[
  {
    "objectID": "labs/lab-2.html",
    "href": "labs/lab-2.html",
    "title": "Lab 2 - College scorecard",
    "section": "",
    "text": "In today’s lab, you’ll use simple linear regression to analyze the relationship between the admissions rate and total cost for colleges and universities in the United States.\n\n\nBy the end of the lab you will…\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use statistical inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "labs/lab-2.html#introduction",
    "href": "labs/lab-2.html#introduction",
    "title": "Lab 2 - College scorecard",
    "section": "",
    "text": "In today’s lab, you’ll use simple linear regression to analyze the relationship between the admissions rate and total cost for colleges and universities in the United States.\n\n\nBy the end of the lab you will…\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use statistical inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "labs/lab-2.html#getting-started",
    "href": "labs/lab-2.html#getting-started",
    "title": "Lab 2 - College scorecard",
    "section": "Getting started",
    "text": "Getting started\n\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-2. It contains the starter documents you need to complete the lab.\nClone the repo and start a new project in RStudio. See the Lab 1 instructions for details on cloning a repo, starting a new R project and configuring git."
  },
  {
    "objectID": "labs/lab-2.html#packages",
    "href": "labs/lab-2.html#packages",
    "title": "Lab 2 - College scorecard",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in today’s lab.\n\nlibrary(tidyverse)  # for data wrangling + visualization\nlibrary(tidymodels) # for modeling\nlibrary(knitr)      # for pretty printing of tables"
  },
  {
    "objectID": "labs/lab-2.html#data-college-scorecard",
    "href": "labs/lab-2.html#data-college-scorecard",
    "title": "Lab 2 - College scorecard",
    "section": "Data: College scorecard",
    "text": "Data: College scorecard\nThe data for this lab is from the scorecard data set in the rcfss R package. It includes information originally obtained from the U.S. Department of Education’s College Scorecard for 1753 colleges and universities during the 2018 - 2019 academic year.\nThe lab focuses on the following variables:\n\nadmrate: Undergraduate admissions rate (from 0-100%)\ncost: The average annual total cost of attendance, including tuition and fees, books and supplies, and living expenses\ntype: Type of college (Public; Private, nonprofit; Private, for-profit)\n\nClick here to see a full list of variables and definitions.\nUse the code below to load the data set.\n\nscorecard &lt;- read_csv(\"data/scorecard.csv\")"
  },
  {
    "objectID": "labs/lab-2.html#exercises",
    "href": "labs/lab-2.html#exercises",
    "title": "Lab 2 - College scorecard",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nNota\n\n\n\nInclude axis labels and an informative title for all plots. Use the kable() function to neatly print tables and regression output.\n\n\n\nExercise 1\nCreate a histogram to examine the distribution of admrate and calculate summary statistics for the center (mean and median) and the spread (standard deviation and IQR).\n\n\nExercise 2\nUse the results from the previous exercise to describe the distribution of admrate. Include the shape, center, spread, and if there are potential outliers.\n\n\nExercise 3\nPlot the distribution of cost and calculate the appropriate summary statistics. Describe the distribution of cost (shape, center, and spread, and outliers) using the plot and appropriate summary statistics.\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nThe goal of this analysis is to fit a regression model that can be used to understand the variability in the cost of college based on the admission rate. Before fitting the model, let’s look at the relationship between the two variables. Create a scatterplot to display the relationship between cost and admissions rate. Describe the relationship between the two variables based on the plot.\n\n\nExercise 5\nDoes the relationship between cost and admissions rate differ by type of college? Modify the plot from the previous exercise visualize the relationship by type of college.\n\n\nExercise 6\nDescribe two new observations from the scatterplot in Exercise 5 that you didn’t see in the scatterplot from Exercise 4.\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 7\nFit the linear regression model. Use the kable function to neatly display the results with a reasonable number of decimals.\n\n\nExercise 8\nConsider the model from the previous exercise.\n\nInterpret the slope in the context of the problem.\nDoes the intercept have a meaningful interpretation? If so, write the interpretation in the context of the problem. Otherwise, explain why the interpretation is not meaningful.\n\n\n\nExercise 9\nConstruct a 95% confidence interval for the slope using bootstrapping. Follow these steps to accomplish this:\n\nFirst set a seed for simulating reproducibly.\nThen, simulate the bootstrap distribution of the slope using 1,000 bootstrap samples.\nThen, visually estimate the bounds of the bootstrap interval based on a histogram of the distribution of the bootstrapped slopes, using the percentile method.\nAnd then, use the get_confidence_interval() function to explicitly calculate the bounds of the confidence interval using the percentile method.\nFinally, interpret the confidence interval in the context of the data.\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 10\nFinally, we want to answer the question “Do the data provide sufficient evidence of a linear relationship between cost and admissions rate, i.e. \\(\\beta_1\\) is different from 0?”\nTo answer this question we will use a hypothesis test. We can conduct a hypothesis test via simulation (what we’ll do in this lab) or using mathematical models (what we’ll do in the next class).\nBefore we can conduct the hypothesis test, let’s first set our hypotheses. Remember that the null hypothesis represents the status quo (nothing going on, i.e. there is no relationship) and the alternative hypothesis represents our research question (there is something going on, i.e. there is a relationship).\n\n\\(H_0\\): There is no linear relationship between the admissions rate and cost of colleges in the United States, \\(\\beta_1 = 0\\)\n\\(H_A\\): There is a linear relationship between the admissions rate and cost of colleges in the United States, \\(\\beta_1 \\ne 0\\)\n\nTo test these hypotheses, we will use a permutation test, where we\n\nSimulate new samples from the original sample via permutation under the assumption that the null hypothesis is true\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the permuted slopes to calculate the p-value for the hypothesis test\n\nThe major difference between constructing a confidence interval and conducting a hypothesis test is that for the hypothesis test we assume that the null hypothesis is true. This requires a simulation scheme that will allow us to measure the natural variability in the data due to sampling but not due to cost and admission rate being correlated by permuting permute one variable to eliminate any existing relationship between the variables. To do so, we randomly assign each admrate value to cost of a given university, i.e. cost and admrate are no longer matched for a given university.\nIn the following code chunk we\n\nFirst set a seed for simulating reproducibly.\nThen, we start with our data frame and specify our model as cost vs. admrate.\nThen, we set our null hypothesis (cost and admrate are independent)\nAnd then we generate 1000 replicates of our data where, for each replicate, we permute values of admrate to randomly assign them to values of cost\nFinally, we fit our model to each of our 1000 permuted datasets\n\n\nset.seed(1234)\n\nperm_fits &lt;- scorecard %&gt;%\n  specify(cost ~ admrate) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  fit()\n\nThe resulting dataset perm_fits has nrow(perm_fits) and ncol(perm_fits) columns. The first column, replicate indicates the replicate number of the dataset the models were fit to; the values in this column range between 1 and 1000. The second column, term, tells us which term (intercept of the model or slope of admrate) the estimate value in the third column is for.\n\nperm_fits\n\n# A tibble: 2,000 × 3\n# Groups:   replicate [1,000]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept  36857. \n 2         1 admrate     -781. \n 3         2 intercept  35901. \n 4         2 admrate      643. \n 5         3 intercept  36608. \n 6         3 admrate     -411. \n 7         4 intercept  35831. \n 8         4 admrate      746. \n 9         5 intercept  36367. \n10         5 admrate      -51.7\n# … with 1,990 more rows\n\n\n\nCreate a histogram of the slope estimates in perm_fits. (Hint: Filter the dataset for just the slope values, term == \"admrate\".)\nEstimate the p-value of the hypothesis test based on this distribution.\nState your conclusion for the test in context.\nIndicate whether or not it is consistent with the results of the hypothesis test from the previous exercise. Briefly explain your response.\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-2.html#submission",
    "href": "labs/lab-2.html#submission",
    "title": "Lab 2 - College scorecard",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nAdvertencia\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-2.html#grading",
    "href": "labs/lab-2.html#grading",
    "title": "Lab 2 - College scorecard",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51\n\n\n\n\n\n1 The “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML."
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Lab 3 - Coffee ratings",
    "section": "",
    "text": "In today’s lab you will analyze data from over 1,000 different coffees to explore the relationship between a coffee’s aroma and it’s overall quality. You will also begin working with your team and practicing a collaborative data analysis workflow.\n\n\nBy the end of the lab you will…\n\nCreate plots and calculate associated statistics to assess model diagnostics.\nPractice collaborating with others using a single Github repo."
  },
  {
    "objectID": "labs/lab-3.html#introduction",
    "href": "labs/lab-3.html#introduction",
    "title": "Lab 3 - Coffee ratings",
    "section": "",
    "text": "In today’s lab you will analyze data from over 1,000 different coffees to explore the relationship between a coffee’s aroma and it’s overall quality. You will also begin working with your team and practicing a collaborative data analysis workflow.\n\n\nBy the end of the lab you will…\n\nCreate plots and calculate associated statistics to assess model diagnostics.\nPractice collaborating with others using a single Github repo."
  },
  {
    "objectID": "labs/lab-3.html#meet-your-team",
    "href": "labs/lab-3.html#meet-your-team",
    "title": "Lab 3 - Coffee ratings",
    "section": "Meet your team!",
    "text": "Meet your team!\nClick here to see the team assignments for STA 210. This will be your team for labs and the final project.\nBefore you get started on the lab, your TA will walk you through the following:\n\nIcebreaker activity to get to know your teammates.\nCome up with a team name. You can’t use the same name as another team, so I encourage you to be creative! Your TA will get your team name by the end of lab.\nFill out the team agreement. This will help you figure out a plan for communication and working together during labs and outside of lab times. You can find the team agreement in the GitHub repo team-agreement-[github_team_name].\nHave one person from the team clone the repo and start a new RStudio project. This person will type the team’s responses as you discuss the sections of the agreement. No one else in the team should type at this point but should be contributing to the discussion.\nBe sure to push the completed agreement to GitHub. Each team member can refer to the document in this repo or download the PDF of the agreement for future reference. You do not need to submit the agreement on Gradescope."
  },
  {
    "objectID": "labs/lab-3.html#getting-started",
    "href": "labs/lab-3.html#getting-started",
    "title": "Lab 3 - Coffee ratings",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-3. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Do not make any changes to the .qmd file until the instructions tell you do to so."
  },
  {
    "objectID": "labs/lab-3.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-3.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 3 - Coffee ratings",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportante\n\n\n\nAssign each person on your team a number 1 through 4. For teams of three, Team Member 1 can take on the role of Team Member 4.\n\n\nThe following exercises must be done in order. Only one person should type in the .qmd file, commit, and push updates at a time. When it is not your turn to type, you should still share ideas and contribute to the team’s discussion.\n\n\n\n\n\n\n⌨️ Team Member 1: Hands on the keyboard.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!1\n\n\n\n1 Don’t trust yourself to keep your hands off the keyboard? Put them in your picket or cross your arms. No matter how silly it might feel, resist the urge to touch your keyboard until otherwise instructed!Change the author to your team name and include each team member’s name in the author field of the YAML in the following format: Team Name: Member 1, Member 2, Member 3, Member 4.\n\nTeam Member 1: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub.\n\n\nTeam Members 2, 3, 4: Once Team Member 1 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the updated name in your .qmd file."
  },
  {
    "objectID": "labs/lab-3.html#packages",
    "href": "labs/lab-3.html#packages",
    "title": "Lab 3 - Coffee ratings",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(ggfortify)"
  },
  {
    "objectID": "labs/lab-3.html#data-coffee-ratings",
    "href": "labs/lab-3.html#data-coffee-ratings",
    "title": "Lab 3 - Coffee ratings",
    "section": "Data: Coffee ratings",
    "text": "Data: Coffee ratings\nThe dataset for this lab comes from the Coffee Quality Database and was obtained from the #TidyTuesday GitHub repo. It includes information about the origin, producer, measures of various characteristics, and the quality measure for over 1000 coffees.\nThis lab will focus on the following variables:\n\naroma: Aroma grade, 0 - 10 scale\ntotal_cup_points: Measure of quality, 0 - 100 scale\n\nYou can find the definitions for all variables in the data set here. Click here for more details about how these measures are obtained.\n\ncoffee_ratings &lt;- read_csv(\"data/coffee_ratings.csv\")"
  },
  {
    "objectID": "labs/lab-3.html#exercises",
    "href": "labs/lab-3.html#exercises",
    "title": "Lab 3 - Coffee ratings",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nNota\n\n\n\n\nInclude axis labels and an informative title for all plots.\nUse the kable function to neatly print tables and regression output. Write all interpretations in the context of the data.\nDo the following exercises in order, following each step carefully.\nOnly one person at a time should type in the .qmd file and push updates.\nIf you are working on any portion of the lab virtually, the person working should share their screen and the others should follow along.\n\n\n\n\n\n\n\n\n\n⌨️ Team Member 1: Hands still on the keyboard. Write the answers to Exercises 1 and 2.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\nExercise 1\nVisualize the relationship between aroma and the total cup points. What do you observe from the plot? Use the plot the describe the relationship between the two variables.\n\n\nExercise 2\nFit the linear model and neatly display the results using 3 digits. Interpret the slope in context of the data.\n\nTeam Member 1: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 2, 3, 4: Once Team Member 1 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercises 1 and 2 in your .qmd file.\n\n\nNow it’s time for a hand off…\n\n\n\n\n\n\n⌨️ Team Member 2: Hands on the keyboard. Write the answers to Exercises 3 and 4.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\n\nExercise 3\nWould the members of your group drink a coffee represented by the intercept? Why or why not? Discuss as a group and write the group’s consensus.\n\n\nExercise 4\nLeverage is the measure of the distance between an observation’s values of the predictor variables and the average values of the predictor variables for the entire data set. An observation s set if have high leverage if its combination of values for the predictor variables is very far from the typical combination of values in the data.An observation has high leverage if its combination of values for the predictor variables is very far from the typical combination of values in the data. Observations with high leverage should be considered as potential influential points.\nWe will proceed assuming the model conditions hold, so let’s focus on the model diagnostics. We’ll start by examining if there are any points with high leverage in the data.\nTheoretically, the leverage of the \\(i^{th}\\) observation as follows:\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j = 1}^n (x_j - \\bar{x})^2}\n\\]\nNote that leverage only depends on values of the predictor variable(s).\nThe sum of the leverages for all points is \\(p + 1\\), where\n\n\\(p\\) is the number of predictors\nIn the case of SLR, \\(\\sum_{i = 1}^n h_i = 2\\)\nThe “typical” leverage is \\(\\frac{(p + 1)}{n}\\)\n\nTherefore, an observation is said to have high leverage if\n\\[\nh_i &gt; \\frac{2(p + 1)}{n}\n\\]\nIn addition to comparing the leverage of points to a threshold, we also generally visualize standard residuals vs. leverage values our data. The autoplot() function from the ggfortify package is very useful for drawing these standard plots easily.\n\nautoplot(coffee_fit$fit, which = 5)\n\n\nWhat threshold will you use to determine if there are points with high leverage for this dataset?\nAre there any observations with high leverage? If so, how many? Briefly explain, including any output, graphs, etc. you used to determine the response. Improve your plot by adding a new year to draw a vertical line (with geom_vline()) at the value of the threshold you’re using to determine which points have high leverage.\n\n\nTeam Member 2: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 3, 4: Once Team Member 2 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercises 3 and 4 in your .qmd file.\n\n\nNow it’s time for another hand off…\n\n\n\n\n\n\n⌨️ Team Member 3: Hands on the keyboard. Write the answers to Exercises 5.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\n\nExercise 5\nAnother standard model diagnostic involves identifying points that don’t fit the pattern from the regression line. We do this by determining which points have large standardized residuals (residual divided by the standard error of residuals).\n\\[\nStd.~res_i = \\frac{y_i - \\hat{y}_1}{\\hat{\\sigma}_\\epsilon ~ \\sqrt{1 - h_i}},\n\\]\nwhere \\(\\hat{\\sigma}_\\epsilon\\) is the regression standard error.\n\n\n\n\n\n\nNota\n\n\n\nThese values are already calculated in the output of augment().\n\n\nObservations that have standardized residuals of large magnitude (usually beyond \\(\\pm\\) 3) are potential outliers, since they don’t fit the pattern determined by the regression model. Therefore, a common practice is to plot standardized residuals vs. fitted values, to make it easier to identify outliers.\nWe can obtain this plot with the following:\n\nautoplot(coffee_fit$fit, which = 3)\n\nCreate this visualization and horizontal lines (with geom_hline()) at the cutoff values for “large” standardized residuals (\\(\\pm\\) 3). Are there any such points in the data? If so, how many? Briefly explain, including any output, graphs, etc. you used to determine the response.\n\nTeam Member 3: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 2, 4: Once Team Member 3 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercise 5 and 4 in your .qmd file.\n\n\nNow it’s time for another hand off…\n\n\n\n\n\n\n⌨️ Team Member 4: Hands on the keyboard. Write the answers to Exercises 6.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\n\n\nExercise 6\nFinally, we’ll examine Cook’s Distance. An observation’s influence on the regression line depends on how close it lies to the general trend of the data (i.e., its standardized residual) and it’s leverage (\\(h_i\\)). Cook’s Distance is a statistic that includes both of these components to measure an observation’s overall impact on the model. Cook’s Distance for the \\(i^{th}\\) observation is defined as the follows:\n\\[\nD_i = \\frac{(std.~res)^2}{p + 1} (\\frac{h_i}{1-\\frac{h_i})\n\\]\nAn observation with large \\(D_i\\) is said to have a strong influence on the predicted values. On that scale,\n\n\\(D_i\\) &gt; 0.5 is moderately influential\n\\(D_i\\) &gt; 1 is very influential\n\nWe can plot of Cook’s distances vs. the observation number with the following:\n\nautoplot(coffee_fit$fit, which = 4, ncol = 1)\n\n\n\n\nStandardized residuals, leverage, and Cook’s Distance should all be examined together. So what do we do with observations identified as outliers or leverage points?\nIt is OK to drop an observation based on the predictor variables if…\n\nIt is meaningful to drop the observation given the context of the problem\nYou intended to build a model on a smaller range of the predictor variables. You should mention this in the write up of the results and be careful to avoid extrapolation when making predictions.\n\nIt is not OK to drop an observation based on the response variable if…\n\nThese are legitimate observations and should be in the model.\nYou can try transformations or increasing the sample size by collecting more data.\n\nSo lastly, let’s analyze Cook’s D to determine if there are influential points in the data.\n\nBased on Cook’s D, are there any influential points in our data? Briefly explain, including any output, graphs, etc. you used to determine the response.\nIf there are influential points, briefly explain why they are outliers, i.e., not in the trend of the rest of the data.\nIf there are influential points, remove those points from the data and refit the model. How do the model coefficients change, if at all?\nIf there are influential points, would you recommend using the model fit with or without these points for inferential conclusions and predictions? Briefly explain why or why not. Additionally, briefly explain potential impacts your choice has on inferential conclusions and/or predictions.\n\n\nTeam Member 4: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 2, 3: Once Team Member 4 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the responses to Exercise 6 and 4 in your .qmd file.\n\n\nNow it’s time for one last hand off…"
  },
  {
    "objectID": "labs/lab-3.html#wrapping-up",
    "href": "labs/lab-3.html#wrapping-up",
    "title": "Lab 3 - Coffee ratings",
    "section": "Wrapping up",
    "text": "Wrapping up\n\n\n\n\n\n\nImportante\n\n\n\n⌨️ Team Member 2: Hands on the keyboard. Make any edits as needed.\n🙅🏽 All other team members: Hands off the keyboard until otherwise instructed!\n\n\n\nTeam Member 2: Render the document and confirm that the changes are visible in the PDF. Then, commit (with an informative commit message) both the .qmd and PDF documents, and finally push the changes to GitHub. Make sure to commit and push all changed files so that your Git pane is empty afterwards.\n\n\nTeam Members 1, 3, 4: Once Team Member 2 is done rendering, committing, and pushing, confirm that the changes are visible on GitHub in your team’s lab repo. Then, in RStudio, click the Pull button in the Git pane to get the updated document. You should see the final version of your .qmd file."
  },
  {
    "objectID": "labs/lab-3.html#submission",
    "href": "labs/lab-3.html#submission",
    "title": "Lab 3 - Coffee ratings",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nAdvertencia\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nSelect one team member to upload the team’s PDF submission to Gradescope.\nBe sure to include every team member’s name in the Gradescope submission.\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”). If any answer spans multiple pages, then mark all pages.\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section.\n\n\n\n\n\n\n\nImportante\n\n\n\nThere should only be one submission per team on Gradescope."
  },
  {
    "objectID": "labs/lab-3.html#grading",
    "href": "labs/lab-3.html#grading",
    "title": "Lab 3 - Coffee ratings",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 6\n42\n\n\nWorkflow & formatting\n52\n\n\nComplete team contract\n3\n\n\n\n\n\n2 The “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML."
  },
  {
    "objectID": "labs/lab-1.html",
    "href": "labs/lab-1.html",
    "title": "Lab 1 - Meet the toolkit",
    "section": "",
    "text": "This lab will go through much of the same workflow we’ve demonstrated in class. The main goal is to reinforce our understanding of R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions.\n\n\n\n\n\n\nNota\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\nAn additional goal is to reinforce git and GitHub, the collaboration and version control system that we will be using throughout the course.\n\n\n\n\n\n\nNota\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\nAs the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\nTo make versioning simpler, this is a solo lab. In the future, you’ll learn about collaborating on GitHub and producing a single lab report for your lab team, but for now, concentrate on getting the basics down.\n\n\nBy the end of the lab, you will…\n\nBe familiar with the workflow using R, RStudio, Git, and GitHub\nGain practice writing a reproducible report using RMarkdown\nPractice version control using GitHub\nBe able to create data visualizations using ggplot2\nBe able to describe variable distributions and the relationship between multiple variables"
  },
  {
    "objectID": "labs/lab-1.html#introduction",
    "href": "labs/lab-1.html#introduction",
    "title": "Lab 1 - Meet the toolkit",
    "section": "",
    "text": "This lab will go through much of the same workflow we’ve demonstrated in class. The main goal is to reinforce our understanding of R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions.\n\n\n\n\n\n\nNota\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\nAn additional goal is to reinforce git and GitHub, the collaboration and version control system that we will be using throughout the course.\n\n\n\n\n\n\nNota\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\nAs the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\nTo make versioning simpler, this is a solo lab. In the future, you’ll learn about collaborating on GitHub and producing a single lab report for your lab team, but for now, concentrate on getting the basics down.\n\n\nBy the end of the lab, you will…\n\nBe familiar with the workflow using R, RStudio, Git, and GitHub\nGain practice writing a reproducible report using RMarkdown\nPractice version control using GitHub\nBe able to create data visualizations using ggplot2\nBe able to describe variable distributions and the relationship between multiple variables"
  },
  {
    "objectID": "labs/lab-1.html#getting-started",
    "href": "labs/lab-1.html#getting-started",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Getting started",
    "text": "Getting started\n\n\n\n\n\n\nImportante\n\n\n\nYour lab TA will lead you through the Getting Started section.\n\n\n\nLog in to RStudio\n\nGo to https://vm-manage.oit.duke.edu/containers and login with your Duke NetID and Password.\nClick STA210 to log into the Docker container. You should now see the RStudio environment.\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nIf you haven’t yet done so, you will need to reserve a container for STA210 first.\n\n\n\n\nSet up your SSH key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps; you are encouraged to follow along as your TA demonstrates the steps.\n\n\n\n\n\n\nNota\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nType credentials::ssh_setup_github() into your console.\nR will ask “No SSH key found. Generate one now?” You should click 1 for yes.\nYou will generate a key. It will begin with “ssh-rsa….” R will then ask “Would you like to open a browser now?” You should click 1 for yes.\nYou may be asked to provide your GitHub username and password to log into GitHub. After entering this information, you should paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., sta210).\n\nYou can find more detailed instructions here if you’re interested.\n\n\nConfigure Git\nThere is one more thing we need to do before getting started on the assignment. Specifically, we need to configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"GitHub username\", \n  user.email = \"Email associated with your GitHub account\"\n  )\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"mine-cetinkaya-rundel\", \n  user.email = \"cetinkaya.mine@gmail.com\"\n  )\n\nYou are now ready interact with GitHub via RStudio!\n\n\nClone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-s22 organization on GitHub. Click on the repo with the prefix lab-1. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ➛ New Project ➛Version Control ➛ Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-1-ikea.qmd to open the template R Markdown file. This is where you will write up your code and narrative for the lab.\n\n\n\nR and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\nYAML\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportante\n\n\n\nOpen the Quarto (`.qmd`) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Rmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab-1.html#packages",
    "href": "labs/lab-1.html#packages",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Packages",
    "text": "Packages\nWe will use the following package in today’s lab.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.1.4     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nThe tidyverse is a meta-package. When you load it you get eight packages loaded for you:\n\nggplot2: for data visualization\ndplyr: for data wrangling\ntidyr: for data tidying and rectangling\nreadr: for reading and writing data\ntibble: for modern, tidy data frames\nstringr: for string manipulation\nforcats: for dealing with factors\npurrr: for iteration with functional programming\n\nThe message that’s printed when you load the package tells you which versions of these packages are loaded as well as any conflicts they may have introduced, e.g., the filter() function from dplyr has now masked (overwritten) the filter() function available in base R (and that’s ok, we’ll use dplyr::filter() anyway).\nWe’ll be using functionality from all of these packages throughout the semester, though we’ll always load them all at once with library(tidyverse). You can find out more about the tidyverse and each of the packages that make it up here."
  },
  {
    "objectID": "labs/lab-1.html#data-ikea-furniture",
    "href": "labs/lab-1.html#data-ikea-furniture",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Data: Ikea furniture",
    "text": "Data: Ikea furniture\nToday’s data is all about Ikea furniture. The data was obtained from the TidyTuesday data collection.\nUse the code below to read in the data.\n\nikea &lt;- read_csv(\"data/ikea.csv\")\n\n\nData dictionary\nThe variable definitions are as follows:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nitem_id\ndouble\nitem id which can be used later to merge with other IKEA data frames\n\n\nname\ncharacter\nthe commercial name of items\n\n\ncategory\ncharacter\nthe furniture category that the item belongs to (Sofas, beds, chairs, Trolleys,…)\n\n\nsellable_online\nlogical\nSellable online TRUE or FALSE\n\n\nlink\ncharacter\nthe web link of the item\n\n\nother_colors\ncharacter\nif other colors are available for the item, or just one color as displayed in the website (Boolean)\n\n\nshort_description\ncharacter\na brief description of the item\n\n\ndesigner\ncharacter\nThe name of the designer who designed the item. this is extracted from the full_description column.\n\n\ndepth\ndouble\nDepth of the item in Centimeter\n\n\nheight\ndouble\nHeight of the item in Centimeter\n\n\nwidth\ndouble\nWidth of the item in Centimeter\n\n\nprice_usd\ndouble\nthe current price in US dollars as it is shown in the website by 4/20/2020\n\n\n\n\n\nView the data\nBefore doing any analysis, you may want to get quick view of the data. This is useful when you’ve imported data to see if your data imported correctly. We can use the view() function to see the entire data set in RStudio. Type the code below in the Console to view the entire dataset.\n\nview(ikea)"
  },
  {
    "objectID": "labs/lab-1.html#exercises",
    "href": "labs/lab-1.html#exercises",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Exercises",
    "text": "Exercises\nWrite all code and narrative in your R Markdown file. Write all narrative in complete sentences. Throughout the assignment, you should periodically Render your Quarto document to produce the updated PDF, commit the changes in the Git pane, and push the updated files to GitHub.\n\n\n\n\n\n\nTip\n\n\n\nMake sure we can read all or your code in your PDF document. This means you will need to break up long lines of code. One way to help avoid long lines of code is is start a new line after every pipe (%&gt;%) and plus sign (+).\n\n\n\nExercise 1\nThe view() function helped us get a quick view of the dataset, but let’s get more detail about its structure. Viewing a summary of the data is a useful starting point for data analysis, especially if the dataset has a large number of observations (rows) or variables (columns). Run the code below to use the glimpse() function to see a summary of the ikea dataset.\nHow many observations are in the ikea dataset? How many variables?\n\nglimpse(ikea)\n\n\n\n\n\n\n\nNota\n\n\n\nIn your lab-1-ikea.qmd document you’ll see that we already added the code required for the exercise as well as a sentence where you can fill in the blanks to report the answer. Use this format for the remaining exercises.\nAlso note that the code chunk as a label: glimpse-data. It’s not required, but good practice and highly encouraged to label your code chunks in this way.\n\n\n\n\nExercise 2\nWe begin each regression analysis with exploratory data analysis (EDA) to help us “get to know” the data and examine the variable distributions and relationships between variables. We do this by visualizing the data and calculating summary statistics to describe the variables in our dataset. In this lab, we will focus on data visualizations.\nLet’s begin by looking at the price of Ikea furniture. Use the code below to visualize the distribution of price_usd, the price in US dollars.\n\nggplot(data = ikea, aes(x = price_usd)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nUse the visualization to describe the distribution of price. In your narrative, include description of the shape, approximate center, approximate spread, and any presence of outliers. Briefly explain why the median is more representative of the center of this distribution than the mean.\n\n\n\n\n\n\nTip\n\n\n\nWhen using the visual editor you can insert a code chunk using the Insert menu on top or by using the catch-all ⌘ / shortcut to insert just about anything. Just execute the shortcut then type what you want to insert. If you are at the beginning of a line you can also enter plain / to invoke the shortcut.\n\n\n\n\nExercise 3\nWhen we make visualizations, we want them to be clear and suitable for a professional audience. This means that, at a minimum, each visualization should have an informative title and informative axis labels. Let’s modify the plot from the previous question to make it suitable for a professional audience. Complete the code below to include an informative title and informative axis labels.\n\nggplot(data = ikea, aes(x = price_usd)) +\n  geom_histogram() +\n  labs(\n    x = \"_____\",\n    y = \"_____\",\n    title = \"_____\"\n  )\n\n\nThis is a good place to fender, commit, and push changes to your remote lab-1 repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message (e.g., “Completed exercises 1 - 3”), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 4\nAnother way to visualize numeric data is using density plots. Make a density plot to visualize the distribution of price_usd. Be sure to include an informative title and informative axis labels.\nIn this course, we’ll be most interested in the relationship between two or more variables, so let’s begin by looking at the distribution of price by category. We’ll focus on the five categories in the code below, since these include commonly purchased types of furniture.\nUse the code below to create a new data frame that only includes the furniture categories of interest. We’re assigning this data frame to an object with a new name, so we don’t overwrite the original data.\nHow many observations are in the ikea_sub dataset? How many variables?\n\nikea_sub &lt;- ikea %&gt;%\n  filter(category %in% c(\n    \"Tables & desks\", \"Beds\",\n    \"Bookcases & shelving units\",\n    \"Sofas & armchairs\", \"Children's furniture\"\n  ))\n\n\n\n\n\n\n\nImportante\n\n\n\nYou will use this newly constructed data frame, ikea_sub, for the remainder of the lab.\n\n\n\n\nExercise 5\nLet’s make a new visualization with the density curves colored by category, so we can compare the distribution of price for each category.\n\nggplot(data = ikea_sub, aes(x = price_usd, fill = category)) +\n  geom_density()\n\n\n\n\nThe overlapping colors make it difficult to tell what’s happening with the distributions for the categories plotted first and hence covered by categories plotted over them. We can change the transparency level of the fill color to help with this. The alpha argument takes values between 0 and 1: 0 is completely transparent and 1 is completely opaque. There is no way to tell what value will work best, so it’s best to try a few.\nRecreate the density plot using a more suitable alpha level, so we can more easily see the distribution of all the categories. Include an informative title and informative axis labels.\n\nggplot(data = ikea_sub, aes(x = price_usd, fill = category)) +\n  geom_density(alpha = 0.8)\n\n\n\n\n\nThis is a good place to render, commit, and push changes to your remote lab-1 repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message (e.g., “Completed exercises 4 and 5”), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 6\nBriefly describe why we defined the fill of the curves by mapping aesthetics of the plot (inside the aes function) but we defined the alpha level as a characteristic of the plotting geom.\n\n\nExercise 7\nOverlapping density plots are not the only way to visualize the relationship between a quantitative and categorical variable.\nUse a different type of plot to visualize the relationship between price_usd and category. Include an informative title and informative axis labels.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the ggplot2 cheatsheet and from Data to Viz for inspiration.\n\n\n\n\nExercise 8\nCompare and contrast your plots from the previous exercise to the overlapping density plots from Exercise 5. What features are apparent in the plot from the previous exercise that aren’t in the overlapping density plots? What features are apparent in the overlapping density plots that aren’t in the plot from the previous exercise? What features are apparent in both?\n\nThis is a good place to render, commit, and push changes to your remote lab-1 repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message (e.g., “Completed exercises 6 - 8”), and push. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\nExercise 9\nNext, let’s look at the relationship between the price and width of Ikea furniture. Fill in the code below to visualize the relationship between the two variables using a scatterplot.\nThen, use your visualization to describe the relationship between the width and price of Ikea furniture.\n\nggplot(data = _____, aes(x = width, y = _____)) +\n  geom_point() + \n  labs(\n    x = \"_____\", \n    y = \"_____\", \n    title = \"_____\"\n    )\n\n\n\nExercise 10\nColor the points of the scatterplot by category. Describe how the relationship between price and width of Ikea furniture differs by category, if at all.\n\nYou’re done and ready to submit your work! Render, commit, and push all remaining changes. You can use the commit message “Done with Lab 1!” , and make sure you have committed and pushed all changed files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "labs/lab-1.html#submission",
    "href": "labs/lab-1.html#submission",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Submission",
    "text": "Submission\nIn this class, we’ll be submitting PDF documents to Gradescope.\n\n\n\n\n\n\nAdvertencia\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-1.html#grading",
    "href": "labs/lab-1.html#grading",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51\n\n\n\n1 The “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML."
  },
  {
    "objectID": "labs/lab-1.html#resources-for-additional-practice-optional",
    "href": "labs/lab-1.html#resources-for-additional-practice-optional",
    "title": "Lab 1 - Meet the toolkit",
    "section": "Resources for additional practice (optional)",
    "text": "Resources for additional practice (optional)\n\nChapter 2: Get Started Data Visualization by Kieran Healy\nChapter 3: Data visualization in R for Data Science by Hadley Wickham\nRStudio Cloud Primers\n\nVisualization Basics: https://rstudio.cloud/learn/primers/1.1\nWork with Data: https://rstudio.cloud/learn/primers/2\nVisualize Data: https://rstudio.cloud/learn/primers/3"
  },
  {
    "objectID": "labs/lab-0.html",
    "href": "labs/lab-0.html",
    "title": "Lab 0 - Meet + greet",
    "section": "",
    "text": "Today’s lab is short and sweet! We just need you to fill out the “Getting to know you” survey. Please go here to take it. You will need to log on to Sakai to access to survey.\nYour answers can be brief. Some of your answers will be used to guide what application examples might be of interest to a majority of students in the course and some of your answers will be used to help guide team formation. We expect this will take you ~10 minutes."
  },
  {
    "objectID": "labs/lab-4.html",
    "href": "labs/lab-4.html",
    "title": "Lab 4 - The Office",
    "section": "",
    "text": "In today’s lab you will analyze data from the schrute package to predict IMDB scores for episodes of The Office.\n\n\n\n\n\n\nNota\n\n\n\nThis is a different data source than the one we’ve used in class last week.\n\n\n\n\nBy the end of the lab you will…\n\nengineer features based on episode scripts\ntrain a model\ninterpret model coefficients\nmake predictions\nevaluate model performance on training and testing data"
  },
  {
    "objectID": "labs/lab-4.html#introduction",
    "href": "labs/lab-4.html#introduction",
    "title": "Lab 4 - The Office",
    "section": "",
    "text": "In today’s lab you will analyze data from the schrute package to predict IMDB scores for episodes of The Office.\n\n\n\n\n\n\nNota\n\n\n\nThis is a different data source than the one we’ve used in class last week.\n\n\n\n\nBy the end of the lab you will…\n\nengineer features based on episode scripts\ntrain a model\ninterpret model coefficients\nmake predictions\nevaluate model performance on training and testing data"
  },
  {
    "objectID": "labs/lab-4.html#getting-started",
    "href": "labs/lab-4.html#getting-started",
    "title": "Lab 4 - The Office",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-4. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo."
  },
  {
    "objectID": "labs/lab-4.html#packages",
    "href": "labs/lab-4.html#packages",
    "title": "Lab 4 - The Office",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(schrute)\nlibrary(lubridate)\nlibrary(knitr)"
  },
  {
    "objectID": "labs/lab-4.html#data-the-office",
    "href": "labs/lab-4.html#data-the-office",
    "title": "Lab 4 - The Office",
    "section": "Data: The Office",
    "text": "Data: The Office\nThe dataset for this lab comes from the schrute package and it’s called theoffice. This dataset contains the entire script transcriptions from The Office.\nLet’s start by taking a peek at the data.\n\nglimpse(theoffice)\n\nRows: 55,130\nColumns: 12\n$ index            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ season           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     &lt;chr&gt; \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         &lt;chr&gt; \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           &lt;chr&gt; \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        &lt;chr&gt; \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha…\n$ text             &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How …\n$ text_w_direction &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      &lt;dbl&gt; 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      &lt;int&gt; 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         &lt;fct&gt; 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005-…\n\n\nThere are 55130 observations and 12 columns in this dataset. The variable names are as follows.\n\nnames(theoffice)\n\n [1] \"index\"            \"season\"           \"episode\"          \"episode_name\"    \n [5] \"director\"         \"writer\"           \"character\"        \"text\"            \n [9] \"text_w_direction\" \"imdb_rating\"      \"total_votes\"      \"air_date\"        \n\n\nEach row in the dataset is a line spoken by a character in a given episode of the show. This means some information at the episode level (e.g., imdb_rating, air_date, etc. are repeated across the rows that belong to a single episode.\nThe air_date variable is coded as a factor, which is undesirable. We’ll want to parse that variable later into its components during feature engineering. So, for now, let’s convert it to date.\n\ntheoffice &lt;- theoffice %&gt;%\n  mutate(air_date = ymd(as.character(air_date)))\n\nLet’s take a look at the data to confirm we’re happy with how each of the variables are encoded.\n\nglimpse(theoffice)\n\nRows: 55,130\nColumns: 12\n$ index            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ season           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ episode_name     &lt;chr&gt; \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\",…\n$ director         &lt;chr&gt; \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis…\n$ writer           &lt;chr&gt; \"Ricky Gervais;Stephen Merchant;Greg Daniels\", \"Ricky…\n$ character        &lt;chr&gt; \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Michael\", \"Micha…\n$ text             &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How …\n$ text_w_direction &lt;chr&gt; \"All right Jim. Your quarterlies look very good. How …\n$ imdb_rating      &lt;dbl&gt; 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6…\n$ total_votes      &lt;int&gt; 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706, 3706,…\n$ air_date         &lt;date&gt; 2005-03-24, 2005-03-24, 2005-03-24, 2005-03-24, 2005…"
  },
  {
    "objectID": "labs/lab-4.html#exercises",
    "href": "labs/lab-4.html#exercises",
    "title": "Lab 4 - The Office",
    "section": "Exercises",
    "text": "Exercises\n\nData prep\n\nExercise 1\nIdentify episodes that touch on Halloween, Valentine’s Day, and Christmas.\n\nFirst, convert all text to lowercase with str_to_lower().\nThen, create three new variables (halloween_mention, valentine_mention, and christmas_mention) where that take on the value 1 if the character string \"halloween\", \"valentine\", or \"christmas\" appears in the text, respectively, and 0 otherwise.\n\nSome code is provided below to help you get started.\n\ntheoffice &lt;- theoffice %&gt;%\n  mutate(\n    text = ___(text),\n    halloween_mention = if_else(str_detect(text, \"___\"), ___, ___),\n    valentine_mention = ___,\n    ___ = ___\n  )\n\n\n\nExercise 2\nIn this exercise we’ll accomplish two separate tasks. And there’s a good reason why we’re doing it all at once; we’re going to drastically change our data frame, from one row per line spoken to one row per episode. We’ll call the resulting data frame office_episodes.\nThe two tasks are as follows:\n\nTask 1. Identify episodes where the word “halloween”, “valentine”, or “christmas” were ever mentioned, using variables you created above.\nTask 2. Calculate the percentage of lines spoken by Jim, Pam, Michael, and Dwight for each episode of The Office.\n\nBelow are some instructions and starter code to get you started with these tasks.\n\nStart by grouping theoffice data by season, episode, episode_name, imdb_rating, total_votes, and air_date. (These variables, except for season have the same value for each given episode, hence grouping by them allows us to make sure they appear in the output of this pipeline.)\nUse summarize() to calculate the desired features at the season-episode level.\nTask 1:\n\nCalculate the number of lines per season per episode, you might name this new variable n_lines.\nThen, calculate the proportion of lines in that episode spoken by each of the four characters Jim, Pam, Michael, and Dwight. Name these new variables lines_jim, lines_pam, lines_michael, and lines_dwight, respectively.\n\nTask 2:\n\nCreate a variable called halloween that sums up the 1s in halloween_mention at the season-episode level and takes on the value \"yes\" if the sum is greater than or equal to 1, or \"no\" otherwise.\nDo something similar for new variables valentine and christmas as well based on values from valentine_mention and christmas_mention.\n\nFinish up your summarize() statement by dropping the groups, so the resulting data frame is no longer grouped and remove n_lines (we won’t use that variable in our analysis, we only calculated it as an intermediary step).\n\n\noffice_episodes &lt;- theoffice %&gt;%\n  group_by(___) %&gt;%\n  summarize(\n    n_lines = n(),\n    lines_jim = sum(character == \"___\") / n_lines,\n    lines_pam = ___,\n    lines_michael = ___,\n    lines_dwight = ___,\n    halloween = if_else(sum(___) &gt;= 1, \"yes\", \"no\"),\n    valentine = if_else(___, \"___\", \"___\"),\n    christmas = if_else(___, \"___\", \"___\"),\n    .groups = \"drop\"\n  ) %&gt;%\n  select(-n_lines)\n\n\n\n\n\n\n\nNota\n\n\n\nWhy summarize() and not mutate()? We use mutate() to add / modify a column of a data frame. The output data frame always has the same number of rows as the input data frame. On the other hand, we use summarize() to reduce the data frame to either a single row (single summary statistic) or one row per each group (summary statistics at the group level).\nAnd what about that .groups argument in summarize? Try running your summarize() step without it first. You’ll see that R print out a message saying “summarize() has grouped output by season, episode. You can override using the .groups argument.” summarize() will only drop the last group. So if you want a data frame that doesn’t have a grouping structure as a result of a summaerize(), you can explicitly ask for that with .groups = \"drop\". Before you proceed, read the documentation for summarize(), and specifically the explanation for the .groups argument to prepare yourself for future instances where you might see this type of message.\n\n\n\n\nExercise 3\nThe Michael Scott character (played by Steve Carrell) left the show at the end of Season 7. Add an indicator variable, michael, that takes on the value \"yes\" if Michael Scott (Steve Carrell) was in the show, and \"no\" if not.\n\noffice_episodes &lt;- office_episodes %&gt;%\n  mutate(michael = if_else(season &gt; ___, \"___\", \"___\"))\n\n\n\nExercise 4\nPrint out the dimensions (dim()) of the new dataset you created as well as the names() of the columns in the dataset.\nYour new dataset, office_episodes, should have 186 rows and 14 columns. The column names should be season, episode, episode_name, imdb_rating, total_votes, air_date, lines_jim, lines_pam, lines_michael, lines_dwight, halloween, valentine, christmas, and michael. If you are not matching these numbers or columns, go back and try to figure out where you went wrong. Or ask your TA for help!\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\nIt’s also a good place to let another team member take over the keyboard! A team member who hasn’t done so yet should pull the changes and make the commits for the next few exercises.\n\n\n\n\nExploratory data analysis\nThis would be a good place to conduct some exploratory data analysis (EDA). For example, plot the proportion of lines spoken by each character over time. Or calculate the percentage of episodes that mention Halloween, or Valentine’s Day, or Christmas. Given we have limited time in the lab we’re not going to ask you to report EDA results as part of this lab, but we’re noting this here to provide suggestions for how you might go about structuring your project.\n\n\nModeling prep\n\nExercise 5\nSplit the data into training (75%) and testing (25%). Save the training and testing data as office_train and office_test respectively.\nNaming suggestion: Call the initial split office_split, the training data office_train, and testing data office_test.\n\nset.seed(123)\noffice_split &lt;- ___(office_episodes)\noffice_train &lt;- ___(office_split)\noffice_test &lt;- ___(___)\n\n\n\nExercise 6\nSpecify a linear regression model with engine \"lm\" and call it office_spec.\nNaming suggestion: Call the model specification office_spec.\n\noffice_spec &lt;- ___\n\n\n\nExercise 7\nCreate a recipe that performs feature engineering using the following steps (in the given order):\n\nupdate_role(): updates the role of episode_name to not be a predictor (be an ID)\nstep_rm(): removes air_date as a predictor\nstep_dummy(): creates dummy variables for all_nominal_predictors()\nstep_zv(): removes all zero variance predictors\n\nNaming suggestion: Call the recipe office_rec.\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) %&gt;%\n  ___\n\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\nIt’s also a good place to let another team member take over the keyboard! A team member who hasn’t done so yet should pull the changes and make the commits for the next few exercises.\n\n\n\nExercise 8\nBuild a model workflow for fitting the model specified earlier and using the recipe you developed to preprocess the data.\nNaming suggestion: Call the model workflow office_wflow.\n\noffice_wflow &lt;- workflow() %&gt;%\n  add_model(___) %&gt;%\n  add_recipe(___)\n\n\n\n\nModel fit and evaluation\n\nExercise 9\nFit the model to training data, neatly display the model output, and interpret two of the slope coefficients.\nNaming suggestion: Call the model fit office_fit.\n\noffice_fit &lt;- office_wflow %&gt;%\n  fit(data = ___)\n\n___\n\n\n\nExercise 10\nCalculate predicted imdb_rating for the training data using the predict() function. Then, bind two columns from the training data to this result: imdb_rating and episode_name. The resulting data frame should have three columns: .pred, imdb_rating, and episode_name. Then, using this data frame, create a scatterplot of predicted and observed IMDB ratings for the training data.\nNaming suggestion: Call the resulting data frame office_train_pred.\nStretch goal. Add episode names, using geom_text(), for episodes with much higher and much lower observed IMDB ratings compared to others.\n\n\nExercise 11\nCalculate the R-squared and RMSE for this model for predictions on the training data.\n\nThis is a good place to render, commit, and push changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty.\nIt’s also a good place to let another team member take over the keyboard! A team member who hasn’t done so yet should pull the changes and make the commits for the next few exercises.\n\n\n\nExercise 12\nRepeat Exercise 10, but with testing data.\nNaming suggestion: Call the resulting data frame office_test_pred.\n\n\nExercise 13\nBased on your visualization on Exercise 12, speculate on whether you expect the R-squared and RMSE for this model to be higher or lower for predictions on the testing data compared to those on the training data, or do you expect them to be the same? Explain your reasoning.\n\n\nExercise 14\nCheck your intuition in Exercise 13 by actually calculating the R-squared and RMSE for this model for predictions on the training data. Comment on whether your intuition is confirmed or not."
  },
  {
    "objectID": "labs/lab-4.html#submission",
    "href": "labs/lab-4.html#submission",
    "title": "Lab 4 - The Office",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nAdvertencia\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-4.html#grading",
    "href": "labs/lab-4.html#grading",
    "title": "Lab 4 - The Office",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51\n\n\n\n\n\n1 The “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML."
  },
  {
    "objectID": "labs/lab-5.html",
    "href": "labs/lab-5.html",
    "title": "Lab 5 - General Social Survey",
    "section": "",
    "text": "In today’s lab you will analyze data from the General Social Survey.\n\n\nBy the end of the lab you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nConduct exploratory data analysis for logistic regression\nInterpret coefficients of logistic regression model"
  },
  {
    "objectID": "labs/lab-5.html#introduction",
    "href": "labs/lab-5.html#introduction",
    "title": "Lab 5 - General Social Survey",
    "section": "",
    "text": "In today’s lab you will analyze data from the General Social Survey.\n\n\nBy the end of the lab you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nConduct exploratory data analysis for logistic regression\nInterpret coefficients of logistic regression model"
  },
  {
    "objectID": "labs/lab-5.html#getting-started",
    "href": "labs/lab-5.html#getting-started",
    "title": "Lab 5 - General Social Survey",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-5. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo."
  },
  {
    "objectID": "labs/lab-5.html#packages",
    "href": "labs/lab-5.html#packages",
    "title": "Lab 5 - General Social Survey",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "labs/lab-5.html#data-general-social-survey",
    "href": "labs/lab-5.html#data-general-social-survey",
    "title": "Lab 5 - General Social Survey",
    "section": "Data: General Social Survey",
    "text": "Data: General Social Survey\nThe General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here.\nThe data for this lab are from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. We will use and abbreviated data set that includes the following variables:\n\nnatmass: Respondent’s answer to the following prompt:\n“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?”\nage: Age in years.\nsex: Sex recorded as male or female\nsei10: Socioeconomic index from 0 to 100\nregion: Region where interview took place\npolviews: Respondent’s answer to the following prompt:\n“We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?”\n\nThe data are in gss2016.csv in the data folder."
  },
  {
    "objectID": "labs/lab-5.html#exercises",
    "href": "labs/lab-5.html#exercises",
    "title": "Lab 5 - General Social Survey",
    "section": "Exercises",
    "text": "Exercises\nThe goal of today’s lab is to use the GSS to examine the relationship between US adults’ political views and attitudes towards government spending on mass transportation projects.\n\nPart I: Exploratory data analysis\n\nLet’s begin by making a binary variable for respondents’ views on spending on mass transportation. Create a new variable that is equal to “1” if a respondent said spending on mass transportation is about right and “0” otherwise. Then make a plot of the new variable, using informative labels for each category.\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data.\n\nMake a plot of the distribution of polviews.\nWhich political view occurs most frequently in this data set?\n\nMake a plot displaying the relationship between satisfaction with mass transportation spending and political views. Use the plot to describe the relationship the two variables.\nWe’d like to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as \"89 or older\".\n\nRecode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values \"89 or older\" with a single value.\nThen plot the distribution of age.\n\n\n\n\nPart II: Logistic regression model\n\nBriefly explain why we should use a logistic regression model to predict the odds a randomly selected person is satisfied with spending on mass transportation.\nLet’s start by fitting a model using the demographic factors - age, sex, sei10, and region - to predict the odds a person is satisfied with spending on mass transportation. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Neatly display the model.\nInterpret the intercept in the context of the data.\nConsider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the odds of being satisfied with spending on mass transportation."
  },
  {
    "objectID": "labs/lab-5.html#submission",
    "href": "labs/lab-5.html#submission",
    "title": "Lab 5 - General Social Survey",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nAdvertencia\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-5.html#grading",
    "href": "labs/lab-5.html#grading",
    "title": "Lab 5 - General Social Survey",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51\n\n\n\n\n\n1 The “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML."
  },
  {
    "objectID": "labs/lab-6.html",
    "href": "labs/lab-6.html",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "",
    "text": "In today’s lab you will analyze data from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”.\n\n\nBy the end of the lab you will be able to…\n\nConduct exploratory data analysis for multinomial logistic regression.\nFit and interpret coefficients of the multinomial logistic regression model.\nUse the multinomial logistic regression model for prediction."
  },
  {
    "objectID": "labs/lab-6.html#introduction",
    "href": "labs/lab-6.html#introduction",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "",
    "text": "In today’s lab you will analyze data from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”.\n\n\nBy the end of the lab you will be able to…\n\nConduct exploratory data analysis for multinomial logistic regression.\nFit and interpret coefficients of the multinomial logistic regression model.\nUse the multinomial logistic regression model for prediction."
  },
  {
    "objectID": "labs/lab-6.html#getting-started",
    "href": "labs/lab-6.html#getting-started",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-s22 organization on GitHub. Click on the repo with the prefix lab-6. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo."
  },
  {
    "objectID": "labs/lab-6.html#packages",
    "href": "labs/lab-6.html#packages",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "labs/lab-6.html#data-five-thirty-eight",
    "href": "labs/lab-6.html#data-five-thirty-eight",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Data: Five Thirty Eight",
    "text": "Data: Five Thirty Eight\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone’s party identification to understand whether a person is a probable voter.\nThe variables we’ll focus on are (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\nrace: Race of respondent, census categories. Note: all categories except Hispanic are non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question “Generally speaking, do you think of yourself as a…”\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\nvoters &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")\n\nNote that the authors use weighting to make the final sample more representative on the US population for their article. We will not use weighting in this assignment, so we should treat the sample as a convenience sample rather than a random sample of the population."
  },
  {
    "objectID": "labs/lab-6.html#exercises",
    "href": "labs/lab-6.html#exercises",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Exercises",
    "text": "Exercises\n\nWhy do you think the authors chose to only include data from people who were eligible to vote for at least four election cycles?\nLet’s prepare the data for analysis and modeling.\n\nThe variable Q30 contains the respondent’s political party identification. Make a new variable that simplifies Q30 into four categories: “Democrat”, “Republican”, “Independent”, “Other” (“Other” also includes respondents who did not answer the question).\nThe variable voter_category identifies the respondent’s past voter behavior. Relevel the variable to make rarely/never the baseline level, followed by sporadic, then always.\n\nIn the FiveThirtyEight article, the authors include visualizations of the relationship between the voter category and demographic variables such as race, age, education, etc. Select two demographic variables. For each variable, interpret the plot to describe its relationship with voter category.\nFit a model using mean-centered age, race, gender, income, and education to predict voter category. Show the code used to fit the model, but do not display the model output.\n\nNext, we want to determine whether party identification be added to the model. In order to do this we need to compare two nested models.\n\nThe reduced model is the one we fit so far, including the predictors mean-centered age, race, gender, income, and education.\nThe full model is the one that includes, in addition to these predictors, party identification.\n\n\nShould party identification be added to the model? Use a drop-in-deviance test to determine if party identification should be added to the model. Include the hypotheses in mathematical notation, the output from the test, and the conclusion in the context of the data. Then, neatly display the model you selected.\n\nUse the model you select for the remainder of the assignment.\n\nInterpret the following coefficients in the context of the data in terms of the odds of voting sporadically versus rarely/never.\n\nInterpret the intercept in the context of the data. Use actual values in the interpretation.\nInterpret the effect of age in the context of the data.\nInterpret the effect of party ID in the context of the data. Include discussion about which level(s) differ from the baseline.\n\nIn the article, the authors write\n\n“Nonvoters were more likely to have lower incomes; to be young; to have lower levels of education; and to say they don’t belong to either political party, which are all traits that square with what we know about people less likely to engage with the political system.”\n\nDoes your model support this statement? Briefly explain why or why not.\nLet’s use the model to predict the voting categories. Obtain the predicted voter category for each observation.\n\nCreate a table of the actual versus predicted voter categories and a visualization of the association between the two.\nHow well did the model perform? Briefly assess the model performance using 2 - 3 observations from the table and/or visualization to support your response."
  },
  {
    "objectID": "labs/lab-6.html#submission",
    "href": "labs/lab-6.html#submission",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nAdvertencia\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-6.html#grading",
    "href": "labs/lab-6.html#grading",
    "title": "Lab 6 - Why Many Americans Don’t Vote",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 10\n45\n\n\nWorkflow & formatting\n51\n\n\n\n\n\n1 The “Workflow & formatting” grade is to assess the reproducible workflow. This includes having at least 3 informative commit messages and updating the name and date in the YAML."
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio containers\n🔗 on Duke Container Manager\n\n\nCourse GitHub organization\n🔗 on GitHub\n\n\nDiscussion forum\n🔗 on Sakai\n\n\nLecture streaming and recordings\n🔗 on Panopto\n\n\nGradebook\n🔗 on Sakai\n\n\nVirtual meetings\n🔗 on Sakai"
  },
  {
    "objectID": "course-bib.html",
    "href": "course-bib.html",
    "title": "Literatura del curso",
    "section": "",
    "text": "El curso tendrá lecturas obligatorias y complementarias clase a clase."
  },
  {
    "objectID": "course-bib.html#introducción-al-big-data-y-a-la-ciencia-social-computacional",
    "href": "course-bib.html#introducción-al-big-data-y-a-la-ciencia-social-computacional",
    "title": "Literatura del curso",
    "section": "Introducción al big data y a la ciencia social computacional",
    "text": "Introducción al big data y a la ciencia social computacional\nSalganik, Matthew J. 2018. Bit by Bit: Social Research in the Digital Age. Princeton, New Jersey: Princeton University Press. Click aquí\nCioffi-Revilla, C. (2014). Introduction to computational social science. London and Hei-delberg: Springer. 2ºEdición. Click aquí\nLazer, David et al 2009. ‘Computational Social Science’. Science 323(5915):721–23. Click aquí\nUrdinez, F., & Cruz, A. (2020). R for Political Data Science: A Practical Guide. CRC Press. Recurso en línea en español: Click aquí"
  },
  {
    "objectID": "course-bib.html#machine-learning-y-ciencias-sociales-computacionales",
    "href": "course-bib.html#machine-learning-y-ciencias-sociales-computacionales",
    "title": "Literatura del curso",
    "section": "Machine learning y ciencias sociales computacionales",
    "text": "Machine learning y ciencias sociales computacionales\nBreiman, L. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199-214. Presentación de estudiantes 1 Click aquí\nGrimmer, J., Roberts, M. E., & Stewart, B. M. (2021). Machine learning for social science: An agnostic approach. Annual Review of Political Science, 24, 395-419. Presentación de estudiantes 2 Click aquí"
  },
  {
    "objectID": "course-bib.html#recolección-manipulación-y-visualización-de-datos",
    "href": "course-bib.html#recolección-manipulación-y-visualización-de-datos",
    "title": "Literatura del curso",
    "section": "Recolección, manipulación y visualización de datos",
    "text": "Recolección, manipulación y visualización de datos\nUrdinez, F., & Cruz, A. (2020). R for Political Data Science: A Practical Guide. CRC Press. Recurso en línea en español: Click aquí\nWickham, H., & Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. ” O’Reilly Media, Inc.”. Recurso en línea: Click aquí\nHealy, K. (2018). Data visualization: a practical introduction. Princeton University Press. Recurso en línea: Click aquí\nWickham, H. (2019). Advanced r. CRC press. Recurso en línea: Click aquí"
  },
  {
    "objectID": "course-bib.html#aplicaciones-a-las-ciencias-sociales-computacionales",
    "href": "course-bib.html#aplicaciones-a-las-ciencias-sociales-computacionales",
    "title": "Literatura del curso",
    "section": "Aplicaciones a las ciencias sociales computacionales",
    "text": "Aplicaciones a las ciencias sociales computacionales\nvan ’t Wout, E., Pieringer, C., Torres Irribarra, D., Asahi, K., & Larroulet, P. (2021). Machine learning for policing: a case study on arrests in Chile. Policing and society, 31(9), 1036-1050. Presentación de estudiantes 3 Click aquí\nRossetti, T., Lobel, H., Rocco, V., & Hurtubia, R. (2019). Explaining subjective perceptions of public spaces as a function of the built environment: A massive data approach. Landscape and urban planning, 181, 169-178. Presentación de estudiantes 4 Click aquí\nMéndez, J. T., Lobel, H., Parra, D., & Herrera, J. C. (2019). Using Twitter to infer user satisfaction with public transport: the case of Santiago, Chile. IEEE Access, 7, 60255-60263. Presentación de estudiantes 5 Click aquí\nBro, N., & Mendoza, M. (2021). Surname affinity in Santiago, Chile: A network-based approach that uncovers urban segregation. PloS one, 16(1), e0244372. Presentación de estudiantes 6 Click aquí\nBronfman, N. C., Repetto, P. B., Guerrero, N., Castañeda, J. V., & Cisternas, P. C. (2021). Temporal evolution in social vulnerability to natural hazards in Chile. Natural hazards, 107(2), 1757-1784. Presentación de estudiantes 7 Click aquí\nBeytía, P., & Müller, H. P. (2022). Towards a Digital Reflexive Sociology: Using Wikipedia’s Biographical Repository as a Reflexive Tool. Poetics, 95, 101732. Presentación de estudiantes 8 Click aquí\nCioffi-Revilla, C. (2014). Introduction to computational social science. London and Hei-delberg: Springer. 2ºEdition. Cap. 4. Click aquí\nLovelace, R., Nowosad, J., & Muenchow, J., (2019), Geocomputation with R, Chapman and Hall/CRC. Click aquí\nSilge, J., & Robinson, D. (2017). Text mining with R: A tidy approach. ” O’Reilly Media, Inc.”. Recurso en línea: Click aquí"
  },
  {
    "objectID": "ae/ae-5-the-office.html",
    "href": "ae/ae-5-the-office.html",
    "title": "AE 5: The Office",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-5-the-office-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-5-the-office.html#packages",
    "href": "ae/ae-5-the-office.html#packages",
    "title": "AE 5: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-5-the-office.html#load-data",
    "href": "ae/ae-5-the-office.html#load-data",
    "title": "AE 5: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\n\nRows: 188 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): title\ndbl  (4): season, episode, imdb_rating, total_votes\ndate (1): air_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "href": "ae/ae-5-the-office.html#exploratory-data-analysis",
    "title": "AE 5: The Office",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nRecreate at least one of the exploratory visualizations from class."
  },
  {
    "objectID": "ae/ae-5-the-office.html#testtrain-split",
    "href": "ae/ae-5-the-office.html#testtrain-split",
    "title": "AE 5: The Office",
    "section": "Test/train split",
    "text": "Test/train split\nSplit your data into testing and training sets."
  },
  {
    "objectID": "ae/ae-5-the-office.html#build-a-recipe",
    "href": "ae/ae-5-the-office.html#build-a-recipe",
    "title": "AE 5: The Office",
    "section": "Build a recipe",
    "text": "Build a recipe\nBuild the recipe from class.\n\nTime permitting…"
  },
  {
    "objectID": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "href": "ae/ae-5-the-office.html#workflows-and-model-fitting",
    "title": "AE 5: The Office",
    "section": "Workflows and model fitting",
    "text": "Workflows and model fitting\nBuild the modeling workflow and fit the model to the training data after feature engineering with the recipe."
  },
  {
    "objectID": "ae/ae-11-volcanoes.html",
    "href": "ae/ae-11-volcanoes.html",
    "title": "AE 11: Multinomial classification",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-11-volcanoes-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#packages",
    "href": "ae/ae-11-volcanoes.html#packages",
    "title": "AE 11: Multinomial classification",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(colorblindr)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#data",
    "href": "ae/ae-11-volcanoes.html#data",
    "title": "AE 11: Multinomial classification",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of on volcanoes. The data come from The Smithsonian Institution via TidyTuesday.\n\nvolcano &lt;- read_csv(here::here(\"ae\", \"data/volcano.csv\"))\n\nRows: 958 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (18): volcano_name, primary_volcano_type, last_eruption_year, country, r...\ndbl  (8): volcano_number, latitude, longitude, elevation, population_within_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst, a bit of data prep:\n\nvolcano &lt;- volcano %&gt;%\n  mutate(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_type = fct_relevel(volcano_type, \"Stratovolcano\", \"Shield\", \"Other\")\n  ) %&gt;%\n  select(\n    volcano_type, latitude, longitude, \n    elevation, tectonic_settings, major_rock_1\n    ) %&gt;%\n  mutate(across(where(is.character), as_factor))"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "href": "ae/ae-11-volcanoes.html#exploratory-data-analysis",
    "title": "AE 11: Multinomial classification",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nCreate a map of volcanoes that is faceted by volcano_type.\n\n\nworld &lt;- map_data(\"world\")\n\nworld_map &lt;- ggplot() +\n  geom_polygon(\n    data = world, \n    aes(\n      x = long, y = lat, group = group),\n      color = \"white\", fill = \"gray50\", \n      size = 0.05, alpha = 0.2\n    ) +\n  theme_minimal() +\n  coord_quickmap() +\n  labs(x = NULL, y = NULL)\n\nworld_map +\n  geom_point(\n    data = volcano,\n    aes(x = longitude, y = latitude,\n        color = volcano_type, \n        shape = volcano_type),\n    alpha = 0.5\n  ) +\n  facet_wrap(~volcano_type) +\n  scale_color_OkabeIto()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#build-a-new-model",
    "href": "ae/ae-11-volcanoes.html#build-a-new-model",
    "title": "AE 11: Multinomial classification",
    "section": "Build a new model",
    "text": "Build a new model\n\nBuild a new model that uses a recipe that includes geographic information (latitude and longitude). How does this model compare to the original? Note:\nUse the same test/train split as well as same cross validation folds. Code for these is provided below.\n\n\n# test/train split\nset.seed(1234)\n\nvolcano_split &lt;- initial_split(volcano)\nvolcano_train &lt;- training(volcano_split)\nvolcano_test  &lt;- testing(volcano_split)\n\n# cv folds\nset.seed(9876)\n\nvolcano_folds &lt;- vfold_cv(volcano_train, v = 5)\nvolcano_folds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [574/144]&gt; Fold1\n2 &lt;split [574/144]&gt; Fold2\n3 &lt;split [574/144]&gt; Fold3\n4 &lt;split [575/143]&gt; Fold4\n5 &lt;split [575/143]&gt; Fold5\n\n\nNew recipe, including geographic information:\n\nvolcano_rec2 &lt;- recipe(volcano_type ~ ., data = volcano_train) %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_predictors())\n\nOriginal model specification and new workflow:\n\nvolcano_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\")\n\nvolcano_wflow2 &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec2) %&gt;%\n  add_model(volcano_spec)\n\nvolcano_wflow2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_other()\n• step_other()\n• step_dummy()\n• step_zv()\n• step_center()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\n\nFit resamples:\n\nvolcano_fit_rs2 &lt;- volcano_wflow2 %&gt;%\n  fit_resamples(\n    volcano_folds, \n    control = control_resamples(save_pred = TRUE)\n    )\n\nvolcano_fit_rs2\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits            id    .metrics         .notes           .predictions      \n  &lt;list&gt;            &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n1 &lt;split [574/144]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n2 &lt;split [574/144]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n3 &lt;split [574/144]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [144 × 7]&gt;\n4 &lt;split [575/143]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;\n5 &lt;split [575/143]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 1]&gt; &lt;tibble [143 × 7]&gt;\n\n\nCollect metrics:\n\ncollect_metrics(volcano_fit_rs2)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.606     5  0.0138 Preprocessor1_Model1\n2 roc_auc  hand_till  0.695     5  0.0245 Preprocessor1_Model1\n\n\nROC curves:\n\nvolcano_fit_rs2 %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(\n    truth = volcano_type,\n    .pred_Stratovolcano:.pred_Other\n  ) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#roc-curves",
    "href": "ae/ae-11-volcanoes.html#roc-curves",
    "title": "AE 11: Multinomial classification",
    "section": "ROC curves",
    "text": "ROC curves\n\nRecreate the ROC curve from the slides.\n\n\nfinal_fit &lt;- last_fit(\n  volcano_wflow2, \n  split = volcano_split\n  )\n\ncollect_predictions(final_fit) %&gt;%\n  roc_curve(truth = volcano_type, .pred_Stratovolcano:.pred_Other) %&gt;%\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +\n  geom_path(size = 1) +\n  scale_color_OkabeIto() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  theme_minimal() +\n  labs(color = NULL)"
  },
  {
    "objectID": "ae/ae-11-volcanoes.html#acknowledgement",
    "href": "ae/ae-11-volcanoes.html#acknowledgement",
    "title": "AE 11: Multinomial classification",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://juliasilge.com/blog/multinomial-volcano-eruptions."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html",
    "href": "ae/ae-4-exam-1-review.html",
    "title": "AE 4: Exam 1 Review",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-4-exam-1-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#packages",
    "href": "ae/ae-4-exam-1-review.html#packages",
    "title": "AE 4: Exam 1 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggfortify)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "href": "ae/ae-4-exam-1-review.html#restaurant-tips",
    "title": "AE 4: Exam 1 Review",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.11 Dahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips &lt;- read_csv(\"data/tip-data.csv\")"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "href": "ae/ae-4-exam-1-review.html#exploratory-analysis",
    "title": "AE 4: Exam 1 Review",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\n\nVisualize, summarize, and describe the relationship between Party and Tip.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#modeling",
    "href": "ae/ae-4-exam-1-review.html#modeling",
    "title": "AE 4: Exam 1 Review",
    "section": "Modeling",
    "text": "Modeling\nLet’s start by fitting a model using Party to predict the Tip at this restaurant.\n\nWrite the statistical model.\nFit the regression line and write the regression equation. Name the model tips_fit and display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret the slope.\nDoes it make sense to interpret the intercept? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#inference",
    "href": "ae/ae-4-exam-1-review.html#inference",
    "title": "AE 4: Exam 1 Review",
    "section": "Inference",
    "text": "Inference\n\nInference for the slope\n\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we’ll focus primarily on the slope in our inference). Describe what each line of code does, supplemented by any visualizations that might help with your description.\n\n\nset.seed(1234)\n\nboot_dist &lt;- tips %&gt;%\n  specify(Tip ~ Party) %&gt;%\n  generate(reps = 100, type = \"bootstrap\") %&gt;%\n  fit()\n\n\nUse the bootstrap distribution created in Exercise 6, boot_dist, to construct a 90% confidence interval for the slope using bootstrapping and the percentile method and interpret it in context of the data.\n\n\n# add your code here\n\n\nConduct a hypothesis test at the equivalent significance level using permutation. State the hypotheses and the significance level you’re using explicitly. Also include a visualization of the null distribution of the slope with the observed slope marked as a vertical line.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercises 7 and 8. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\nNow repeat Exercises 7 and 8 using approaches based on mathematical models.\n\n\n# add your code here\n\n\nCheck the relevant conditions for Exercise 9. Are there any violations in conditions that make you reconsider your inferential findings?\n\n\n# add your code here\n\n\n\nInference for a prediction\n\nBased on your model, predict the tip for a party of 4.\n\n\n# add your code here\n\n\nSuppose you’re asked to construct a confidence and a prediction interval for your finding in Exercise 11. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals from Exercise 12 and comment on whether your guess is confirmed.\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "href": "ae/ae-4-exam-1-review.html#model-diagnostics",
    "title": "AE 4: Exam 1 Review",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nLeverage (Outliers in x direction)\n\nWhat is the threshold used to identify observations with high leverage? Calculate the threshold and save the value as leverage_threshold.\n\n\n# add your code here\n\n\nMake a plot of the standardized residuals vs. leverage (you can do this with ggplot() or with autoplot(which = 5)). Use geom_vline() to add a vertical line to help identify points with high leverage.\n\n\n# add your code here\n\n\nLet’s dig into the data further. Which observations have high leverage? Why do these points have high leverage?\n\n\n# add your code here\n\n\n\nIdentifying outliers (outliers in y direction)\n\nMake a plot of the residuals vs. fitted values and a plot of the square root of the absolute value of standardized residuals vs. fitted (You can use autoplot(which = c(1, 3)) to display the plots side-by-side).\n\n\nHow are the plots similar? How do they differ?\nWhat is an advantage of using the plot of the residuals vs. fitted to check conditions and model diagnostics?\nWhat is an advantage of using the plot of the \\(\\sqrt{|\\text{standardized residuals}|}\\) vs. fitted to check conditions and model diagnostics?\n\n\n# add your code here\n\n\nAre there any observations that are outliers?\n\n\n# add your code here\n\n\n\nCook’s distance\n\nMake a plot to check Cook’s distance (autoplot(which = 4)). Based on this plot, are there any points that have a strong influence on the model coefficients?\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "href": "ae/ae-4-exam-1-review.html#adding-another-variable",
    "title": "AE 4: Exam 1 Review",
    "section": "Adding another variable",
    "text": "Adding another variable\n\nAdd another variable, Alcohol, to your exploratory visualization. Describe any patterns that emerge.\n\n\n# add your code here\n\n\nFit a multiple linear regression model predicting Tip from Party and Alcohol. Display the results with kable() and a reasonable number of digits.\n\n\n# add your code here\n\n\nInterpret each of the slopes.\nDoes it make sense to interpret the intercept? Explain your reasoning.\nAccording to this model, is the rate of change in tip amount the same for various sizes of parties regardless of alcohol consumption or are they different? Explain your reasoning."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html",
    "href": "ae/ae-6-the-office-cv.html",
    "title": "AE 6: The Office",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-6-the-office-cv-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#packages",
    "href": "ae/ae-6-the-office-cv.html#packages",
    "title": "AE 6: The Office",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#load-data",
    "href": "ae/ae-6-the-office-cv.html#load-data",
    "title": "AE 6: The Office",
    "section": "Load data",
    "text": "Load data\n\noffice_episodes &lt;- read_csv(\"data/office_episodes.csv\")"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "href": "ae/ae-6-the-office-cv.html#split-data-into-training-and-testing",
    "title": "AE 6: The Office",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#specify-model",
    "href": "ae/ae-6-the-office-cv.html#specify-model",
    "title": "AE 6: The Office",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-recipe",
    "href": "ae/ae-6-the-office-cv.html#create-recipe",
    "title": "AE 6: The Office",
    "section": "Create recipe",
    "text": "Create recipe\nCreate the recipe from class. Call it office_rec1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#create-workflow",
    "href": "ae/ae-6-the-office-cv.html#create-workflow",
    "title": "AE 6: The Office",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe. Call it office_wflow1.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#cross-validation",
    "href": "ae/ae-6-the-office-cv.html#cross-validation",
    "title": "AE 6: The Office",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "href": "ae/ae-6-the-office-cv.html#summarize-cv-metrics",
    "title": "AE 6: The Office",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "href": "ae/ae-6-the-office-cv.html#another-model---model-2",
    "title": "AE 6: The Office",
    "section": "Another model - Model 2",
    "text": "Another model - Model 2\nCreate a different (simpler, involving fewer variables) recipe and call it office_rec2. Conduct 10-fold cross validation and summarize metrics. Describe how the two models compare to each other based on cross validation metrics."
  },
  {
    "objectID": "ae/ae-8-rail-trail.html",
    "href": "ae/ae-8-rail-trail.html",
    "title": "AE 8: Rail Trail",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-8-rail-trail-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#packages-and-data",
    "href": "ae/ae-8-rail-trail.html#packages-and-data",
    "title": "AE 8: Rail Trail",
    "section": "Packages and data",
    "text": "Packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrail_trail &lt;- read_csv(\"data/rail_trail.csv\")"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-1",
    "href": "ae/ae-8-rail-trail.html#exercise-1",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model predicting volume from hightemp and season.\n\nrt_mlr_main_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic       p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   -125.       71.7     -1.75  0.0841       \n2 hightemp         7.54      1.17     6.43  0.00000000692\n3 seasonSpring     5.13     34.3      0.150 0.881        \n4 seasonSummer   -76.8      47.7     -1.61  0.111        \n\n\nRecreate the following visualization which displays the three regression lines we can draw based on the results of this model.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-2",
    "href": "ae/ae-8-rail-trail.html#exercise-2",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 2",
    "text": "Exercise 2\nAdd an interaction effect between hightemp and season and comment on the significance of the interaction predictors. Time permitting, visualize the interaction model as well.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-8-rail-trail.html#exercise-3",
    "href": "ae/ae-8-rail-trail.html#exercise-3",
    "title": "AE 8: Rail Trail",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a model predicting volume from all available predictors.\n\nrt_full_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(volume ~ ., data = rail_trail)\n\ntidy(rt_full_fit)\n\n# A tibble: 8 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\nRecreate the following visualization which displays a histogram of residuals and a normal density curve overlaid.\n\n\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html",
    "href": "ae/ae-1-dcbikeshare.html",
    "title": "AE 02: Bike rentals in DC",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-1-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-1-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 02: Bike rentals in DC",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#data",
    "href": "ae/ae-1-dcbikeshare.html#data",
    "title": "AE 02: Bike rentals in DC",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-and-temperature",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts and temperature",
    "text": "Daily counts and temperature\n\nExercise 1\nVisualize the distribution of daily bike rentals and temperature as well as the relationship between these two variables.\n\nggplot(bikeshare, aes(x = count)) +\n  geom_histogram(binwidth = 250)\n\n\n\nggplot(bikeshare, aes(y = count, x = temp_orig)) +\n  geom_point()\n\n\n\n\n\n\nExercise 2\nDescribe the distribution of daily bike rentals and the distribution of temperature based on the visualizations created in Exercise 1. Include the shape, center, spread, and presence of any potential outliers.\n[Add your answer here]\n\n\nExercise 3\nThere appears to be one day with a very small number of bike rentals. What was the day? Why were the number of bike rentals so low on that day? Hint: You can Google the date to figure out what was going on that day.\n[Add your answer here]\n\n\nExercise 4\nDescribe the relationship between daily bike rentals and temperature based on the visualization created in Exercise 1. Comment on how we expect the number of bike rentals to change as the temperature increases.\n[Add your answer here]\n\n\nExercise 5\nSuppose you want to fit a model so you can use the temperature to predict the number of bike rentals. Would a model of the form\n\\[\\text{count} = \\beta_0 + \\beta_1 ~ \\text{temp\\_orig} + \\epsilon\\]\nbe the best fit for the data? Why or why not?\nNo."
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-1-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 02: Bike rentals in DC",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 6\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 7\nNext, let’s look at how the daily bike rentals differ by season. Let’s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a “smoothed out histogram”. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 8\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 9\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#modeling",
    "href": "ae/ae-1-dcbikeshare.html#modeling",
    "title": "AE 02: Bike rentals in DC",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 10\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 11\nUsing the data you filtered in Exercise 10, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here\n\n\n\nExercise 12\nUse the output to write out the estimated regression equation.\n[Add your answer here]\n\n\nExercise 13\nInterpret the slope in the context of the data.\n[Add your answer here]\n\n\nExercise 14\nInterpret the intercept in the context of the data.\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-1-dcbikeshare.html#synthesis",
    "href": "ae/ae-1-dcbikeshare.html#synthesis",
    "title": "AE 02: Bike rentals in DC",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 15\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]\n\nThe following exercises will be completed only if time permits.\n\n\nExercise 16\nPick another season. Based on the visualization in Exercise 8, would you expect the slope of the relationship between temperature and daily bike rentals to be smaller or larger than the slope of the model you’ve been working with so far? Explain your reasoning.\n[Add your answer here]\n\n\nExercise 17\nFor this season you picked in Exercise 16, fit a linear model for predicting daily bike rentals from temperature. Note, you will need to filter your data for this season first. Use the output to write out the estimated regression equation and interpret the slope and the intercept of this model.\n\n# add your code here\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html",
    "href": "ae/ae-12-exam-3-review.html",
    "title": "AE 12: Exam 3 Review",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-12-exam-3-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#packages",
    "href": "ae/ae-12-exam-3-review.html#packages",
    "title": "AE 12: Exam 3 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data)\nlibrary(rms)\nlibrary(nnet)"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#data",
    "href": "ae/ae-12-exam-3-review.html#data",
    "title": "AE 12: Exam 3 Review",
    "section": "Data",
    "text": "Data\nAs part of a study of the effects of predatory intertidal crab species on snail populations, researchers measured the mean closing forces and the propodus heights of the claws on several crabs of three species.\n\n\nclaws &lt;- read_csv(here::here(\"ae\", \"data/claws.csv\"))\n\nWe will use the following variables:\n\nforce: Closing force of claw (newtons)\nheight: Propodus height (mm)\nspecies: Crab species - Cp(Cancer productus), Hn (Hemigrapsus nudus), Lb(Lophopanopeus bellus)\nlb: 1 if Lophopanopeus bellus species, 0 otherwise\nhn: 1 if Hemigrapsus nudus species, 0 otherwise\ncp: 1 if Cancer productus species, 0 otherwise\nforce_cent: mean centered force\nheight_cent: mean centered height\n\nBefore we get started, let’s make the categorical and indicator variables factors.\n\nclaws &lt;- claws %&gt;%\n  mutate(\n    species = as_factor(species),\n    lb = as_factor(lb),\n    hn = as_factor(hn),\n    cp = as_factor(cp)\n  )"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "href": "ae/ae-12-exam-3-review.html#probabilities-vs.-odds-vs.-log-odds",
    "title": "AE 12: Exam 3 Review",
    "section": "Probabilities vs. odds vs. log-odds",
    "text": "Probabilities vs. odds vs. log-odds\nWhy we use log-odds as response variable: https://sta210-s22.github.io/website/slides/lec-18.html#/do-teenagers-get-7-hours-of-sleep"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-1",
    "href": "ae/ae-12-exam-3-review.html#exercise-1",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 1",
    "text": "Exercise 1\nFill in the blanks:\n\nUse log-odds to fit the model (outcome)\nUse odds to interpret model results\nUse probabilities to make predictions for individual observations and ultimately to make classification decisions"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-2",
    "href": "ae/ae-12-exam-3-review.html#exercise-2",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 2",
    "text": "Exercise 2\nSuppose we want to use force to determine whether or not a crab is from the Lophopanopeus bellus (Lb) species. Why should we use a logistic regression model for this analysis?\n\nclaws %&gt;%\n  distinct(lb)\n\n# A tibble: 2 × 1\n  lb   \n  &lt;fct&gt;\n1 0    \n2 1"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-3",
    "href": "ae/ae-12-exam-3-review.html#exercise-3",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe will use the mean-centered variables for force in the model. The model output is below. Write the equation of the model produced by R. Don’t forget to fill in the blanks for ….\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.798\n0.358\n-2.233\n0.026\n-1.542\n-0.123\n\n\nforce_cent\n0.043\n0.039\n1.090\n0.276\n-0.034\n0.123\n\n\n\n\n\nLet \\(\\pi\\) be probability that a crab is from Lb species.\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\Big) = -0.798 + 0.043 * force\\_cent\n\\]"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-4",
    "href": "ae/ae-12-exam-3-review.html#exercise-4",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 4",
    "text": "Exercise 4\nInterpret the intercept in the context of the data.\n\nmean_force &lt;- round(mean(claws$force), 2)\n\nFor crabs with average closing force (12.13 newtons), we expect odds of the crab being Lophopanopeus bellus is 0.45 (exp(-0.798))."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-5",
    "href": "ae/ae-12-exam-3-review.html#exercise-5",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret the effect of force in the context of the data.\nWhen x goes up by 1 unit, we expect y to change by (slope) units.\nFor each additional unit increase in closing force, the odds of crab being from lb species multiplies on average by a factor of 1.0439379."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-6",
    "href": "ae/ae-12-exam-3-review.html#exercise-6",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 6",
    "text": "Exercise 6\nNow let’s consider adding height_cent to the model. Fit the model that includes height_cent. Then use AIC to choose the model that best fits the data.\n\nlb_fit_2 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(lb ~ force_cent + height_cent, data = claws)\n\ntidy(lb_fit_2, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -1.13     0.463      -2.44  0.0146  -2.17      -0.306\n2 force_cent     0.211    0.0925      2.28  0.0227   0.0563     0.424\n3 height_cent   -0.895    0.398      -2.25  0.0245  -1.82      -0.234\n\nglance(lb_fit_1)$AIC\n\n[1] 50.19535\n\nglance(lb_fit_2)$AIC\n\n[1] 44.11812"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-7",
    "href": "ae/ae-12-exam-3-review.html#exercise-7",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 7",
    "text": "Exercise 7\nWhat do the following mean in the context of this data. Explain and calculate them.\n\nSensitivity: P(predict lb | actual lb) = 6 / 12\nSpecificity: P(predict not lb | actual not lb) = 4/ 26\nNegative predictive power: P(actual not lb | predict not lb) = 22 / 28\nPositive predictive power: P(actual lb | predict lb) = 6 / 10\n\n\n\n\nActual\nPredict lb\nPredict not lb\nTOTAL predicted\n\n\n\n\nLb\n6\n6\n12\n\n\nNot lb\n4\n22\n26\n\n\nTOTAL actual\n10\n28\n38"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-8",
    "href": "ae/ae-12-exam-3-review.html#exercise-8",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 8",
    "text": "Exercise 8\nWrite the equation of the model.\n\\[\\log\\Big(\\frac{\\hat{\\pi}_{Hn}}{\\hat{\\pi}_{Cp}}\\Big) = \\]\n\\[\\log\\Big(\\frac{\\hat{\\pi}_{Lb}}{\\hat{\\pi}_{Cp}}\\Big) = \\]"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-9",
    "href": "ae/ae-12-exam-3-review.html#exercise-9",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nInterpret the intercept for the odds a crab is Hn vs. Cp species.\nInterpret the effect of force on the odds a crab is Lb vs. Cp species."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-10",
    "href": "ae/ae-12-exam-3-review.html#exercise-10",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 10",
    "text": "Exercise 10\nInterpret the effect of force on the odds a crab is in the Hn vs. Lb species.\nCAUTION: We can write an interpretation based on the estimated coefficients; however, we can’t make any inferential conclusions for this question based on the current model. We would need to refit the model with Lb as the baseline category to do so."
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#exercise-11",
    "href": "ae/ae-12-exam-3-review.html#exercise-11",
    "title": "AE 12: Exam 3 Review",
    "section": "Exercise 11",
    "text": "Exercise 11\nConditions for multinomial logistic (and logistic models as well):\n\nIndependence:\nRandomness:\nLinearity:\n\n\nemplogitplot1(lb ~ force, data = claws, ngroups = 10)\nemplogitplot1(lb ~ height, data = claws, ngroups = 10)\n\n\n\n\n\n\n\n\n\n\n\n\n# add code here for other species here\n\n\n\n\n\n# add code here for other species here"
  },
  {
    "objectID": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "href": "ae/ae-12-exam-3-review.html#checking-for-multicollinearity-in-logistic-and-multinomial-logistic",
    "title": "AE 12: Exam 3 Review",
    "section": "Checking for multicollinearity in logistic and multinomial logistic",
    "text": "Checking for multicollinearity in logistic and multinomial logistic\nSimilar to multiple linear regression, we can also check for multicollinearity in logistic and multinomial logistic models.\n\nUse the vif function to check for multicollinearity in logistic regression.\n\n\n\n\n\nThe vif function doesn’t work for the multinomial logistic regression models, so we can look at a correlation matrix of the predictors as a way to assess if the predictors are highly correlated:"
  },
  {
    "objectID": "ae/ae-9-odds.html",
    "href": "ae/ae-9-odds.html",
    "title": "AE 9: Odds",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-9-odds-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-9-odds.html#packages",
    "href": "ae/ae-9-odds.html#packages",
    "title": "AE 9: Odds",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(here::here(\"ae\", \"data/framingham.csv\")) %&gt;%\n  select(totChol, TenYearCHD) %&gt;%\n  drop_na() %&gt;%\n  mutate(high_risk = as.factor(TenYearCHD)) %&gt;%\n  select(totChol, high_risk)"
  },
  {
    "objectID": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "href": "ae/ae-9-odds.html#linear-regression-vs.-logistic-regression",
    "title": "AE 9: Odds",
    "section": "Linear regression vs. logistic regression",
    "text": "Linear regression vs. logistic regression\nState whether a linear regression model or logistic regression model is more appropriate for each scenario:\n\nUse age and education to predict if a randomly selected person will vote in the next election.\nUse budget and run time (in minutes) to predict a movie’s total revenue.\nUse age and sex to calculate the probability a randomly selected adult will visit Duke Health in the next year."
  },
  {
    "objectID": "ae/ae-9-odds.html#heart-disease",
    "href": "ae/ae-9-odds.html#heart-disease",
    "title": "AE 9: Odds",
    "section": "Heart disease",
    "text": "Heart disease\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the total cholesterol to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\ntotChol: total cholesterol (mg/dL)\n\n\n\nOutcome: high_risk\n\nggplot(data = heart_disease, aes(x = high_risk)) + \n  geom_bar() + \n  scale_x_discrete(labels = c(\"1\" = \"High risk\", \"0\" = \"Low risk\")) +\n  labs(\n    title = \"Distribution of 10-year risk of heart disease\", \n    x = NULL)\n\n\n\n\n\nheart_disease %&gt;%\n  count(high_risk)\n\n# A tibble: 2 × 2\n  high_risk     n\n  &lt;fct&gt;     &lt;int&gt;\n1 0          3555\n2 1           635\n\n\n\n\nCalculating probability and odds\n\nWhat is the probability a randomly selected person in the study is not high risk for heart disease?\nWhat are the odds a randomly selected person in the study is not high risk for heart disease?\n\n\n\nLogistic regression model\nFit a logistic regression model to understand the relationship between total cholesterol and risk for heart disease.\nLet \\(pi\\) be the probability an adult is high risk. The statistical model is\n\\[\\log\\Big(\\frac{\\pi_i}{1-\\pi_i}\\Big) = \\beta_0 + \\beta_1 TotChol_i\\]\n\nheart_disease_fit &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(high_risk ~ totChol, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) %&gt;% kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.894\n0.230\n-12.607\n0\n\n\ntotChol\n0.005\n0.001\n5.268\n0\n\n\n\n\n\n\nWrite the regression equation. Round to 3 digits.\n\n\n\nCalculating log-odds, odds and probabilities\nBased on the model, if a randomly selected person has a total cholesterol of 250 mg/dL,\n\nWhat are the log-odds they are high risk for heart disease?\nWhat are the odds they are high risk for heart disease?\nWhat is the probability they are high risk for heart disease? Use the odds to calculate your answer.\n\n\n\nComparing observations\nSuppose a person’s cholesterol changes from 250 mg/dL to 200 mg/dL.\n\nHow do you expect the log-odds that this person is high risk for heart disease to change?\nHow do you expect the odds that this person is high risk for heart disease to change?"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html",
    "href": "ae/ae-7-exam-2-review.html",
    "title": "AE 7: Exam 2 Review",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-7-exam-2-review-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#packages",
    "href": "ae/ae-7-exam-2-review.html#packages",
    "title": "AE 7: Exam 2 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\n# fix data!\nloans_full_schema &lt;- droplevels(loans_full_schema)"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#goal",
    "href": "ae/ae-7-exam-2-review.html#goal",
    "title": "AE 7: Exam 2 Review",
    "section": "Goal",
    "text": "Goal\nCreate a model for precicting interest_rate."
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#view-data",
    "href": "ae/ae-7-exam-2-review.html#view-data",
    "title": "AE 7: Exam 2 Review",
    "section": "View data",
    "text": "View data\nNote the dimensions of the data and the variable names. Review the data dictionary.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "href": "ae/ae-7-exam-2-review.html#split-data-into-training-and-testing",
    "title": "AE 7: Exam 2 Review",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#write-the-model",
    "href": "ae/ae-7-exam-2-review.html#write-the-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Write the model",
    "text": "Write the model\nWrite the model for predicting interest rate (interest_rate) from debt to income ratio (debt_to_income), the term of loan (term), the number of inquiries (credit checks) into the applicant’s credit during the last 12 months (inquiries_last_12m), whether there are any bankruptcies listed in the public record for this applicant (bankrupt), and the type of application (application_type). The model should allow for the effect of to income ratio on interest rate to vary by application type.\nAdd model here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#exploration",
    "href": "ae/ae-7-exam-2-review.html#exploration",
    "title": "AE 7: Exam 2 Review",
    "section": "Exploration",
    "text": "Exploration\nExplore characteristics of the variables you’ll use for the model using the training data only.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#specify-model",
    "href": "ae/ae-7-exam-2-review.html#specify-model",
    "title": "AE 7: Exam 2 Review",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it office_spec.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-recipe",
    "href": "ae/ae-7-exam-2-review.html#create-recipe",
    "title": "AE 7: Exam 2 Review",
    "section": "Create recipe",
    "text": "Create recipe\n\nPredict interest_rate from debt_to_income, term, inquiries_last_12m, public_record_bankrupt, and application_type.\nMean center debt_to_income.\nMake term a factor.\nCreate a new variable: bankrupt that takes on the value “no” if public_record_bankrupt is 0 and the value “yes” if public_record_bankrupt is 1 or higher. Then, remove public_record_bankrupt.\nInteract application_type with debt_to_income.\nCreate dummy variables where needed and drop any zero variance variables.\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#create-workflow",
    "href": "ae/ae-7-exam-2-review.html#create-workflow",
    "title": "AE 7: Exam 2 Review",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#cross-validation",
    "href": "ae/ae-7-exam-2-review.html#cross-validation",
    "title": "AE 7: Exam 2 Review",
    "section": "Cross validation",
    "text": "Cross validation\nConduct 10-fold cross validation.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "href": "ae/ae-7-exam-2-review.html#summarize-cv-metrics",
    "title": "AE 7: Exam 2 Review",
    "section": "Summarize CV metrics",
    "text": "Summarize CV metrics\nSummarize metrics from your CV resamples.\n\n# add code here\n\nWhy are we focusing on R-squared and RMSE instead of adjusted R-squared, AIC, BIC?\n[Add response here]"
  },
  {
    "objectID": "ae/ae-7-exam-2-review.html#next-steps",
    "href": "ae/ae-7-exam-2-review.html#next-steps",
    "title": "AE 7: Exam 2 Review",
    "section": "Next steps…",
    "text": "Next steps…\nDepending on time, either\n\nCreate a workflow for another model with a new recipe (omitting the interaction variable), conduct CV, do model selection between these two, and then interpret the coefficients for the selected model.\nOr interpret the coefficients for the one model you fit.\n\nMake sure to interpret the intercept and slope coefficient for at least one numerical, one categorical, and one interaction predictor."
  },
  {
    "objectID": "ae/ae-0-movies.html",
    "href": "ae/ae-0-movies.html",
    "title": "Movie budgets and revenues",
    "section": "",
    "text": "Importante\n\n\n\nThis application exercise is a demo only. You do not have a corresponding repository for it and you’re not expected to turn in anything for it.\nWe will look at the relationship between budget and revenue for movies made in the United States in 1986 to 2020. The dataset is created based on data from the Internet Movie Database (IMDB).\nlibrary(tidyverse) # for data analysis and visualisation\nlibrary(scales)    # for pretty axis labels\nlibrary(DT)        # for interactive table"
  },
  {
    "objectID": "ae/ae-0-movies.html#data",
    "href": "ae/ae-0-movies.html#data",
    "title": "Movie budgets and revenues",
    "section": "Data",
    "text": "Data\nThe movies data set includes basic information about each movie including budget, genre, movie studio, director, etc. A full list of the variables may be found here.\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/danielgrijalva/movie-stats/master/movies.csv\")\n\nView the first 10 rows of data.\n\nmovies\n\n# A tibble: 7,668 × 15\n   name   rating genre  year released score  votes director writer star  country\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n 1 The S… R      Drama  1980 June 13…   8.4 9.27e5 Stanley… Steph… Jack… United…\n 2 The B… R      Adve…  1980 July 2,…   5.8 6.5 e4 Randal … Henry… Broo… United…\n 3 Star … PG     Acti…  1980 June 20…   8.7 1.2 e6 Irvin K… Leigh… Mark… United…\n 4 Airpl… PG     Come…  1980 July 2,…   7.7 2.21e5 Jim Abr… Jim A… Robe… United…\n 5 Caddy… R      Come…  1980 July 25…   7.3 1.08e5 Harold … Brian… Chev… United…\n 6 Frida… R      Horr…  1980 May 9, …   6.4 1.23e5 Sean S.… Victo… Bets… United…\n 7 The B… R      Acti…  1980 June 20…   7.9 1.88e5 John La… Dan A… John… United…\n 8 Ragin… R      Biog…  1980 Decembe…   8.2 3.3 e5 Martin … Jake … Robe… United…\n 9 Super… PG     Acti…  1980 June 19…   6.8 1.01e5 Richard… Jerry… Gene… United…\n10 The L… R      Biog…  1980 May 16,…   7   1   e4 Walter … Bill … Davi… United…\n# … with 7,658 more rows, and 4 more variables: budget &lt;dbl&gt;, gross &lt;dbl&gt;,\n#   company &lt;chr&gt;, runtime &lt;dbl&gt;\n\n\nThe ___ dataset has ___ observations and ___ variables."
  },
  {
    "objectID": "ae/ae-0-movies.html#analysis",
    "href": "ae/ae-0-movies.html#analysis",
    "title": "Movie budgets and revenues",
    "section": "Analysis",
    "text": "Analysis\n\nGross over time\nWe begin by looking at how the average gross revenue (gross) has changed over time. Since we want to visualize the results, we will choose a few genres of interest for the analysis.\n\ngenre_list &lt;- c(\"Comedy\", \"Action\", \"Animation\", \"Horror\")\n\nThen, we will filter for these genres and visualize the average gross revenue over time.\n\nmovies %&gt;%\n  filter(genre %in% genre_list) %&gt;% \n  group_by(genre,year) %&gt;%\n  summarise(avg_gross = mean(gross)) %&gt;%\n  ggplot(mapping = aes(x = year, y = avg_gross, color= genre)) +\n    geom_point() + \n    geom_line() +\n    scale_color_viridis_d() +\n    scale_y_continuous(labels = label_dollar()) +\n    labs(\n      x = \"Year\",\n      y = \"Average Gross Revenue (US Dollars)\",\n      color = \"Genre\",\n      title = \"Gross Revenue Over Time\"\n    )\n\n`summarise()` has grouped output by 'genre'. You can override using the\n`.groups` argument.\n\n\nWarning: Removed 47 rows containing missing values (geom_point).\n\n\nWarning: Removed 23 row(s) containing missing values (geom_path).\n\n\n\n\n\nThe plot suggests …\n\n\nBudget and gross\nNext, let’s see the relationship between a movie’s budget and its gross revenue.\n\nmovies %&gt;%\n  filter(genre %in% genre_list, budget &gt; 0) %&gt;% \n  ggplot(mapping = aes(x=log(budget), y = log(gross), color=genre)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ genre) + \n  scale_color_viridis_d() +\n  labs(\n    x = \"Log-transformed Budget\",\n    y = \"Log-transformed Gross Revenue\"\n  )\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 35 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 35 rows containing missing values (geom_point)."
  },
  {
    "objectID": "ae/ae-0-movies.html#exercises",
    "href": "ae/ae-0-movies.html#exercises",
    "title": "Movie budgets and revenues",
    "section": "Exercises",
    "text": "Exercises\n\nSuppose we fit a regression model for each genre that uses budget to predict gross revenue. What are the signs of the correlation between budget and gross and the slope in each regression equation?\nSuppose we fit the regression model from the previous question. Which genre would you expect to have the smallest residuals, on average (residual = observed revenue - predicted revenue)?\nIn the remaining time, discuss the following: Notice in the graph above that budget and gross are log-transformed. Why are the log-transformed values of the variables displayed rather than the original values (in U.S. dollars)?"
  },
  {
    "objectID": "ae/ae-0-movies.html#appendix",
    "href": "ae/ae-0-movies.html#appendix",
    "title": "Movie budgets and revenues",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of genres in the data set:\n\nmovies %&gt;% \n  distinct(genre) %&gt;%\n  arrange(genre) %&gt;% \n  datatable()"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html",
    "href": "ae/ae-3-duke-forest.html",
    "title": "AE 3: Duke Forest houses",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-3-duke-forest-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#packages",
    "href": "ae/ae-3-duke-forest.html#packages",
    "title": "AE 3: Duke Forest houses",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "href": "ae/ae-3-duke-forest.html#predict-sale-price-from-area",
    "title": "AE 3: Duke Forest houses",
    "section": "Predict sale price from area",
    "text": "Predict sale price from area\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "ae/ae-3-duke-forest.html#model-conditions",
    "href": "ae/ae-3-duke-forest.html#model-conditions",
    "title": "AE 3: Duke Forest houses",
    "section": "Model conditions",
    "text": "Model conditions\n\nExercise 1\nThe following code produces the residuals vs. fitted values plot for this model. Comment out the layer that defines the y-axis limits and re-create the plot. How does the plot change? Why might we want to define the limits explicitly?\n\ndf_aug &lt;- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n\n\n\n\n\n\nExercise 2\nImprove how the values on the axes of the plot are displayed by modifying the code below.\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html",
    "href": "ae/ae-2-dcbikeshare.html",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-2-dcbikeshare-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "href": "ae/ae-2-dcbikeshare.html#bike-rentals-in-dc",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Bike rentals in DC",
    "text": "Bike rentals in DC\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#data",
    "href": "ae/ae-2-dcbikeshare.html#data",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")\n\nSee AE 1 for the first part of this analysis."
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "href": "ae/ae-2-dcbikeshare.html#daily-counts-temperature-and-season",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Daily counts, temperature, and season",
    "text": "Daily counts, temperature, and season\n\nExercise 1\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n# add code developed during livecoding here\n\n\n\nExercise 2\nNext, let’s look at how the daily bike rentals differ by season. Let’s visualize the distribution of bike rentals by season using density plots. You can think of a density plot as a “smoothed out histogram”. Compare and contrast the distributions. Is this what you expected? Why or why not?\n\n# add code developed during livecoding here\n\n[Add your answer here]\n\n\nExercise 3\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n# add code developed during livecoding here\n\n\n\nExercise 4\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#modeling",
    "href": "ae/ae-2-dcbikeshare.html#modeling",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Modeling",
    "text": "Modeling\n\nExercise 5\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n# add code developed during livecoding here\n\n\n\nExercise 6\nUsing the data you filtered in Exercise 5, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n# add code developed during livecoding here"
  },
  {
    "objectID": "ae/ae-2-dcbikeshare.html#synthesis",
    "href": "ae/ae-2-dcbikeshare.html#synthesis",
    "title": "AE 2: Bike rentals in DC (continued)",
    "section": "Synthesis",
    "text": "Synthesis\n\nExercise 7\nSuppose you work for a bike share company in Durham, NC, and they want to predict daily bike rentals in 2022. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html",
    "href": "ae/ae-10-flight-delays.html",
    "title": "AE 10: Flight delays",
    "section": "",
    "text": "Importante\n\n\n\nGo to the course GitHub organization and locate the repo titled ae-10-flight-delays-YOUR_GITHUB_USERNAME to get started."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#packages",
    "href": "ae/ae-10-flight-delays.html#packages",
    "title": "AE 10: Flight delays",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#data",
    "href": "ae/ae-10-flight-delays.html#data",
    "title": "AE 10: Flight delays",
    "section": "Data",
    "text": "Data\nFor this application exercise we will work with a dataset of 25,000 randomly sampled flights that departed one of three NYC airports (JFK, LGA, EWR) in 2013.\n\nflight_data &lt;- read_csv(\"data/flight-data.csv\")\n\nRows: 25000 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): origin, dest, carrier, arr_delay\ndbl  (4): dep_time, flight, air_time, distance\ndttm (1): time_hour\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nConvert arr_delay to factor with levels \"late\" (first level) and \"on_time\" (second level). This variable is our outcome and it indicates whether the flight’s arrival was more than 30 minutes.\n\n\nflight_data &lt;- flight_data %&gt;%\n  mutate(arr_delay = as.factor(arr_delay))\n\nlevels(flight_data$arr_delay)\n\n[1] \"late\"    \"on_time\"\n\n\n\nLet’s get started with some data prep: Convert all variables that are character strings to factors.\n\n\n#flight_data &lt;- flight_data %&gt;%\n#  mutate(\n#    origin = as.factor(origin),\n#    carrier = as.factor(carrier),\n#    dest = as.factor(dest)\n#    )\n\nflight_data &lt;- flight_data %&gt;%\n  #go across all columns and convert that are characters to factors\n  #go across all columns and convert if is.character = TRUE to factors\n  #go across all columns and if is.character apply as.factor\n  mutate(across(where(is.character), as.factor))"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#modeling-prep",
    "href": "ae/ae-10-flight-delays.html#modeling-prep",
    "title": "AE 10: Flight delays",
    "section": "Modeling prep",
    "text": "Modeling prep\n\nSplit the data into testing (75%) and training (25%), and save each subset.\n\n\nset.seed(222)\n\nflight_split &lt;- initial_split(flight_data)\n\nflight_train &lt;- training(flight_split)\nflight_test &lt;- testing(flight_split)\n\n\nSpecify a logistic regression model that uses the \"glm\" engine.\n\n\nflight_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nNext, we’ll create two recipes and workflows and compare them to each other."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "href": "ae/ae-10-flight-delays.html#model-1-everything-and-the-kitchen-sink",
    "title": "AE 10: Flight delays",
    "section": "Model 1: Everything and the kitchen sink",
    "text": "Model 1: Everything and the kitchen sink\n\nDefine a recipe that predicts arr_delay using all variables except for flight and time_hour, which, in combination, can be used to identify a flight. Also make sure this recipe handles dummy coding as well as issues that can arise due to having categorical variables with some levels apparent in the training set but not in the testing set. Call this recipe flights_rec1.\n\n\nflights_rec1 &lt;- recipe(arr_delay ~ ., data = flight_train) %&gt;%\n  update_role(flight, time_hour, new_role = \"id\") %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nCreate a workflow that uses flights_rec1 and the model you specified.\n\n\nflight_wflow1 &lt;- workflow() %&gt;%\n  add_recipe(flights_rec1) %&gt;%\n  add_model(flight_spec)\n\n\nFit the this model to the training data using your workflow and display a tidy summary of the model fit.\n\n\nflight_fit1 &lt;- flight_wflow1 %&gt;%\n  fit(data = flight_train)\n\ntidy(flight_fit1)\n\n# A tibble: 119 × 5\n   term          estimate   std.error statistic   p.value\n   &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  13.3      287.          0.0464  9.63e-  1\n 2 dep_time     -0.00164    0.0000504 -32.6     1.04e-233\n 3 air_time     -0.0349     0.00179   -19.5     1.75e- 84\n 4 distance      0.00533    0.00523     1.02    3.08e-  1\n 5 date          0.000227   0.000198    1.15    2.51e-  1\n 6 origin_JFK    0.0830     0.102       0.815   4.15e-  1\n 7 origin_LGA   -0.0360     0.0983     -0.366   7.14e-  1\n 8 dest_ACK    -12.4      287.         -0.0434  9.65e-  1\n 9 dest_ALB    -12.4      287.         -0.0433  9.65e-  1\n10 dest_ANC     -3.75     928.         -0.00404 9.97e-  1\n# … with 109 more rows\n\n\n\nPredict arr_delay for the testing data using this model.\n\n\nflight_aug1 &lt;- augment(flight_fit1, flight_test)\n\n\nPlot the ROC curve and find the area under the curve. Comment on how well you think this model has done for predicting arrival delay.\n\n\nflight_aug1 %&gt;%\n  roc_curve(\n    truth = arr_delay,\n    .pred_late\n  ) %&gt;%\n  autoplot()\n\n\n\nflight_aug1 %&gt;%\n  roc_auc(\n    truth = arr_delay,\n    .pred_late\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.734"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "href": "ae/ae-10-flight-delays.html#model-2-lets-be-a-bit-more-thoughtful",
    "title": "AE 10: Flight delays",
    "section": "Model 2: Let’s be a bit more thoughtful",
    "text": "Model 2: Let’s be a bit more thoughtful\n\nDefine a new recipe, flights_rec2, that, in addition to what was done in flights_rec1, adds features for day of week and month based on date and also adds indicators for all US holidays (also based on date). A list of these holidays can be found in timeDate::listHolidays(\"US\"). Once these features are added, date should be removed from the data. Then, create a new workflow, fit the same model (logistic regression) to the training data, and do predictions on the testing data. Finally, draw another ROC curve and find the area under the curve. Compare the predictive performance of this new model to the previous one. Based on the area under the curve statistic, which model does better?"
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "href": "ae/ae-10-flight-delays.html#putting-it-altogether",
    "title": "AE 10: Flight delays",
    "section": "Putting it altogether",
    "text": "Putting it altogether\n\nCreate an ROC curve that plots both models, in different colors, and adds a legend indicating which model is which."
  },
  {
    "objectID": "ae/ae-10-flight-delays.html#acknowledgement",
    "href": "ae/ae-10-flight-delays.html#acknowledgement",
    "title": "AE 10: Flight delays",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis exercise was inspired by https://www.tidymodels.org/start/recipes/."
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#data-sources",
    "href": "project-tips-resources.html#data-sources",
    "title": "Project tips + resources",
    "section": "",
    "text": "R Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\n\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\nSuppress Code, Warnings, & Messages\n\nInclude the following code in a code chunk at the top of your .qmd file to suppress all code, warnings, and other messages. Use the code chunk header {r set-up, include = FALSE} to suppress this set up code.\n\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE, \n                      message = FALSE)\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n❌ NO! The x-axis is hard to read because the names overlap.\n\nggplot(data = mpg, aes(x = manufacturer)) +\n  geom_bar()\n\n\n\n\n✅ YES! Names are readable\n\nggplot(data = mpg, aes(y = manufacturer)) +\n  geom_bar()\n\n\n\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg %&gt;%\n  count(manufacturer) %&gt;%\n  mutate(manufacturer = str_to_title(manufacturer)) %&gt;%\n  ggplot(aes(y = fct_reorder(manufacturer,n), x = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") +\n  theme_minimal() \n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources\n\nR for Data Science\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Descripción del curso",
    "section": "",
    "text": "El concepto de políticas basadas en evidencia adquiere cada vez más relevancia en distintos sectores dedicados a el análisis y diseño de políticas públicas, por lo que se hace necesario contar con profesionales preparados para estos efectos.\nLa era digital ha producido una explosión de datos conductuales y relacionales de distinta naturaleza comúnmente denominados big data. La ciencia social computacional (CSC) utiliza herramientas computacionales para procesar, analizar, visualizar y modelar estos datos. Este curso introduce a 2 áreas de interés para la CSC: 1) la extracción y manipulación de bases de datos y 2) técnicas aplicadas al análisis de datos geográficos (GD), redes sociales (SNA) y el procesamiento del lenguaje natural (NLP).\nLas clases estarán programas de la siguiente manera:\n\n\n\n\nFecha\nHorario\nContenidos\nMateriales\n\n\n\n\nSesión 1\nViernes 12/05/2023\n18:30 - 21:45\nIntroducción a las CSC para políticas públcas\n🖥️\n\n\nSesión 2\nViernes 26/05/2023\n18:30 - 21:45\nIntroducción a las CSC para políticas públcas\n🖥️\n\n\nSesión 3\nViernes 09/06/2023\n18:30 - 21:45\nFundamentos de la estadística para las CSC\n\n\n\nSesión 4\nSábado 10/06/2023\n09:00 - 12:15\nExtracción automática de texto: Webscraping y APIs\n\n\n\nSesión 5\nViernes 30/06/2023\n18:30 - 21:45\nIntroducción al análisis de redes sociales\n\n\n\nSesión 6\nViernes 14/07/2023\n18:30 - 21:45\nAnálisis de datos geográficos\n\n\n\nSesión 7\nViernes 04/08/2023\n18:30 - 21:45\nAnálisis automatizado de texto\n\n\n\nSesión 8\nSábado 05/08/2023\n09:00 - 12:15\nPresentación de trabajos finales"
  },
  {
    "objectID": "index.html#descripción-del-curso",
    "href": "index.html#descripción-del-curso",
    "title": "Descripción del curso",
    "section": "",
    "text": "El concepto de políticas basadas en evidencia adquiere cada vez más relevancia en distintos sectores dedicados a el análisis y diseño de políticas públicas, por lo que se hace necesario contar con profesionales preparados para estos efectos.\nLa era digital ha producido una explosión de datos conductuales y relacionales de distinta naturaleza comúnmente denominados big data. La ciencia social computacional (CSC) utiliza herramientas computacionales para procesar, analizar, visualizar y modelar estos datos. Este curso introduce a 2 áreas de interés para la CSC: 1) la extracción y manipulación de bases de datos y 2) técnicas aplicadas al análisis de datos geográficos (GD), redes sociales (SNA) y el procesamiento del lenguaje natural (NLP).\nLas clases estarán programas de la siguiente manera:\n\n\n\n\nFecha\nHorario\nContenidos\nMateriales\n\n\n\n\nSesión 1\nViernes 12/05/2023\n18:30 - 21:45\nIntroducción a las CSC para políticas públcas\n🖥️\n\n\nSesión 2\nViernes 26/05/2023\n18:30 - 21:45\nIntroducción a las CSC para políticas públcas\n🖥️\n\n\nSesión 3\nViernes 09/06/2023\n18:30 - 21:45\nFundamentos de la estadística para las CSC\n\n\n\nSesión 4\nSábado 10/06/2023\n09:00 - 12:15\nExtracción automática de texto: Webscraping y APIs\n\n\n\nSesión 5\nViernes 30/06/2023\n18:30 - 21:45\nIntroducción al análisis de redes sociales\n\n\n\nSesión 6\nViernes 14/07/2023\n18:30 - 21:45\nAnálisis de datos geográficos\n\n\n\nSesión 7\nViernes 04/08/2023\n18:30 - 21:45\nAnálisis automatizado de texto\n\n\n\nSesión 8\nSábado 05/08/2023\n09:00 - 12:15\nPresentación de trabajos finales"
  },
  {
    "objectID": "index.html#objetivos-de-aprendizaje",
    "href": "index.html#objetivos-de-aprendizaje",
    "title": "Descripción del curso",
    "section": "Objetivos de aprendizaje",
    "text": "Objetivos de aprendizaje\nAl terminar el curso, los estudiantes sabrán:\n\nIdentificar tipos de datos para distintas preguntas de investigación.\nIdentificar ventajas y desventajas de distintos métodos empíricos.\nAplicar métodos de ciencia de datos utilizando análisis estadístico\n\nLos puntos anteriores se realizarán utilizando R, por lo que los/as estudiantes profundizarán su conocimiento de este lenguaje de programación."
  },
  {
    "objectID": "index.html#metodología-del-curso",
    "href": "index.html#metodología-del-curso",
    "title": "Descripción del curso",
    "section": "Metodología del curso",
    "text": "Metodología del curso\nEl curso es práctico por lo que tiene el propósito de enseñar a programar orientado proyectos, por lo que tendremos actividades:\n\nInteractivas: los estudiantes deberán programar análisis a partir de una pregunta/objetivo de investigación\nDe análisis: los estudiantes deberán leer investigaciones en el área y exponer a sus compañeros los aspectos principales\nAplicación: los estudiantes deberán aplicar alguna de las técnicas vistas a una pregunta relevante para las políticas públicas"
  },
  {
    "objectID": "index.html#evaluaciones",
    "href": "index.html#evaluaciones",
    "title": "Descripción del curso",
    "section": "Evaluaciones",
    "text": "Evaluaciones\nEl curso tendrá 4 evaluaciones ponderadas y organizadas de la siguiente manera:\n\n\n\n\nFecha Publicación\nFecha - Horario de entrega\nContenidos\nPautas\n\n\n\n\nTarea 1\n30/05/2023\n25/06/2023 - 23:59\nPropuesta proyecto de investigación\nClick aquí\n\n\nTarea 2\n30/06/2023\n16/07/2023 - 23:59\nAjuste a la propuesta + metodología\nClick aquí\n\n\nProyecto\n30/06/2023\n05/08/2023 - 18:30 a 21:45\nPresentación de resultados\nClick aquí\n\n\nPresentaciones\n-\nLink aquí\nPresentación de artículos\nClick aquí\n\n\n\nTodas las evaluaciones se deben enviar por correo al profesor José Daniel Conejeros jdconejeros@uc.cl con copia a la ayudante Sofía Madariaga Alvarado samadariaga@uc.cl."
  },
  {
    "objectID": "index.html#integridad-académica",
    "href": "index.html#integridad-académica",
    "title": "Descripción del curso",
    "section": "Integridad académica",
    "text": "Integridad académica\nComo miembro de la comunidad de la Pontificia Universidad Católica de Chile, me comprometo a respetar los principios y normativas que la rigen. Asimismo, me comprometo a actuar con rectitud y honestidad en las relaciones con los demás integrantes de la comunidad y en la realización de todo trabajo, particularmente en aquellas actividades vinculadas a la docencia, al aprendizaje y la creación, difusión y transferencia del conocimiento. Además, me comprometo a velar por la dignidad e integridad de las personas, evitando incurrir en y, rechazando, toda conducta abusiva de carácter físico, verbal, psicológico y de violencia sexual. Del mismo modo, asumo el compromiso de cuidar los bienes de la Universidad.\nMás detalles sobre la integridad académica UC aquí: https://integridadacademica.uc.cl/"
  },
  {
    "objectID": "index.html#contribuciones-y-agradecimientos",
    "href": "index.html#contribuciones-y-agradecimientos",
    "title": "Descripción del curso",
    "section": "Contribuciones y agradecimientos",
    "text": "Contribuciones y agradecimientos\nEl contenido de este curso fue desarrollado y adaptado a partir del trabajo del Ph.D. Naim Bro Profesor asistente de la Universidad Adolfo Ibañez y Ph.D. en Sociología de la universidad de Cambridge.\n\nPuedes acceder al curso de referencia aquí: Data policy\n\nEl formato web de este curso fue desarrollado y adaptado a partir del trabajo de la Ph.D. Mine Çetinkaya-Rundel Profesora y Directora de Estudios de Grado del Departamento de Ciencias Estadísticas de la Universidad de Duke.\n\nPuedes acceder al curso de referencia aquí: STA 210"
  },
  {
    "objectID": "course-team.html#profesor",
    "href": "course-team.html#profesor",
    "title": "Equipo docente",
    "section": "Profesor",
    "text": "Profesor\nEl ayudante estará a cargo de a) entregar las clases a los estudiantes, b) realizar las correcciones de las evaluaciones c) contacto con estudiantes.\n\nJosé Daniel Conejeros\n jdconejeros@uc.cl\n JDConejeros\n jd-conejeros.com\nLicenciado en Ciencias Sociales, Magíster (c) en Sociología y Estadística PUC. Sus temas de investigación actual son salud pública, cambio climático, pobreza y metodologías cuantitativas. Ha participado en proyectos de investigación en educación, género y actualmente pertenece al Nucleo Milenio de Resistencia Bacteriana (Microb-R). En el ámbito profesional, ha sido consultor en temas cuantitativos para distintas fundaciones y organizaciones sin fines de lucro."
  },
  {
    "objectID": "course-team.html#ayudante",
    "href": "course-team.html#ayudante",
    "title": "Equipo docente",
    "section": "Ayudante",
    "text": "Ayudante\nEl ayudante estará a cargo de a) ofrecer sesiones de apoyo a los estudiantes y b) realizar las correcciones de las tareas escritas.\n\nSofía Madariaga\n sofia.madariaga@mail.udp.cl\n s-madariaga\n Sociología, Pontificia Universidad Católica\n Temas de interés: Educación, consumo cultural y aplicaciones computacionales a la estadística"
  },
  {
    "objectID": "supplemental/mlr-matrix.html",
    "href": "supplemental/mlr-matrix.html",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "",
    "text": "Nota\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides the details for the matrix notation for multiple linear regression. We assume the reader has familiarity with some linear algebra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of linear algebra."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#introduction",
    "href": "supplemental/mlr-matrix.html#introduction",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Ecuación 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Ecuación 2\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "href": "supplemental/mlr-matrix.html#matrix-representation-for-the-regression-model",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Matrix Representation for the Regression Model",
    "text": "Matrix Representation for the Regression Model\nWe can represent the Ecuación 1 and Ecuación 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "href": "supplemental/mlr-matrix.html#estimating-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Estimating the Coefficients",
    "text": "Estimating the Coefficients\nThe least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes\n\\[\n\\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})\n\\tag{5}\\]\nwhere \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\).\n\\[\n(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} -\n\\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} +\n\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}})\n\\tag{6}\\]\nNote that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, Ecuación 7 becomes\n\\[\n\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\n\\hat{\\boldsymbol{\\beta}}\n\\tag{7}\\]\nSince we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes Ecuación 5, will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} & = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\\n&\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\\n& \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\\n& \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}}\n\\end{aligned}\n\\tag{8}\\]\nThus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\)."
  },
  {
    "objectID": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "href": "supplemental/mlr-matrix.html#variance-covariance-matrix-of-the-coefficients",
    "title": "Matrix Notation for Multiple Linear Regression",
    "section": "Variance-covariance matrix of the coefficients",
    "text": "Variance-covariance matrix of the coefficients\nWe will use two properties to derive the form of the variance-covariance matrix of the coefficients:\n\n\\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\)\n\nFirst, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\)\n\\[\n\\begin{aligned}\nE[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &= E \\begin{bmatrix}\\epsilon_1  & \\epsilon_2 & \\dots & \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}  \\\\\n& = E \\begin{bmatrix} \\epsilon_1^2  & \\epsilon_1 \\epsilon_2 & \\dots & \\epsilon_1 \\epsilon_n \\\\\n\\epsilon_2 \\epsilon_1 & \\epsilon_2^2 & \\dots & \\epsilon_2 \\epsilon_n \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n \\epsilon_1 & \\epsilon_n \\epsilon_2 & \\dots & \\epsilon_n^2\n\\end{bmatrix} \\\\\n& = \\begin{bmatrix} E[\\epsilon_1^2]  & E[\\epsilon_1 \\epsilon_2] & \\dots & E[\\epsilon_1 \\epsilon_n] \\\\\nE[\\epsilon_2 \\epsilon_1] & E[\\epsilon_2^2] & \\dots & E[\\epsilon_2 \\epsilon_n] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n \\epsilon_1] & E[\\epsilon_n \\epsilon_2] & \\dots & E[\\epsilon_n^2]\n\\end{bmatrix}\n\\end{aligned}\n\\tag{9}\\]\nRecall, the regression assumption that the errors \\(\\epsilon_i's\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write Ecuación 9 as\n\\[\nE[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T]  = \\begin{bmatrix} \\sigma^2  & 0 & \\dots & 0 \\\\\n0 & \\sigma^2  & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\sigma^2\n\\end{bmatrix} = \\sigma^2 \\mathbf{I}\n\\tag{10}\\]\nwhere \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix.\nNext, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\).\nRecall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then,\n\\[\n\\begin{aligned}\n\\hat{\\boldsymbol{\\beta}} &= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\\n&= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\\n&= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\\n\\end{aligned}\n\\tag{11}\\]\nUsing these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\)\n\\[\n\\begin{aligned}\nE[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n& = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n&  = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\\n\\end{aligned}\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html",
    "href": "supplemental/model-diagnostics-matrix.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Nota\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression. Please see Matrix Form of Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#introduction",
    "href": "supplemental/model-diagnostics-matrix.html#introduction",
    "title": "Model Diagnostics",
    "section": "Introduction",
    "text": "Introduction\nSuppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in Ecuación 1.\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\tag{1}\\]\nWe can write the response for the \\(i^{th}\\) observation as shown in Ecuación 2.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i\n\\tag{2}\\]\nsuch that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\)."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "href": "supplemental/model-diagnostics-matrix.html#matrix-form-for-the-regression-model",
    "title": "Model Diagnostics",
    "section": "Matrix Form for the Regression Model",
    "text": "Matrix Form for the Regression Model\nWe can represent the Ecuación 1 and Ecuación 2 using matrix notation. Let\n\\[\n\\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix}\n\\hspace{15mm}\n\\mathbf{X} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}\n\\hspace{15mm}\n\\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}\n\\tag{3}\\]\nThus,\n\\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\]\nTherefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\n\\tag{4}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "href": "supplemental/model-diagnostics-matrix.html#hat-matrix-leverage",
    "title": "Model Diagnostics",
    "section": "Hat Matrix & Leverage",
    "text": "Hat Matrix & Leverage\nRecall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following:\n\\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\n\\tag{5}\\]\nCombining Ecuación 4 and Ecuación 5, we can write \\(\\hat{\\mathbf{Y}}\\) as the following:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} &= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n&= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\\n\\end{aligned}\n\\tag{6}\\]\nWe define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus Ecuación 6 becomes\n\\[\n\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\n\\tag{7}\\]\nThe diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as\n\\[\nh_{ii} =  \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\n\\]\nWe call these diagonal elements, the leverage of each observation.\nThe diagonal elements of the hat matrix have the following properties:\n\n\\(0 \\leq h_ii \\leq 1\\)\n\\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model.\nThe mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\).\n\nUsing these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.\nWhen there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from Ecuación 4 using Ecuación 7.\n\\[\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt]\n& = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt]\n&= (1-\\mathbf{H})\\mathbf{Y}\n\\end{aligned}\n\\tag{8}\\]\nNote that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H})\\) is also idempotent. These matrices are also symmetric. Using these properties and Ecuación 8, we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is\n\\[\n\\begin{aligned}\nVar(\\mathbf{e}) &= \\mathbf{e}\\mathbf{e}^T \\\\[10pt]\n&=  (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt]\n&= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H})  \\\\[10pt]\n&= \\hat{\\sigma}^2(1-\\mathbf{H})\n\\end{aligned}\n\\tag{9}\\]\nwhere \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage."
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "href": "supplemental/model-diagnostics-matrix.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form\n\\[\nstd.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\n\\]\nThe expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From Ecuación 9), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore,\n\\[\nstd.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}\n\\tag{10}\\]"
  },
  {
    "objectID": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "href": "supplemental/model-diagnostics-matrix.html#cooks-distance",
    "title": "Model Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nCook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as\n\\[\nD_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}}\n\\tag{11}\\]\nwhere \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since Ecuación 12 below is mathematically equivalent to Ecuación 11.\n\\[\nD_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg]\n\\tag{12}\\]"
  },
  {
    "objectID": "supplemental/slr-derivations.html",
    "href": "supplemental/slr-derivations.html",
    "title": "Deriving the Least-Squares Estimates for Simple Linear Regression",
    "section": "",
    "text": "Nota\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\n\n\nThis document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals, as shown in Ecuación 1.\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\tag{1}\\]\nRecall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize /eq-ssr by taking the partial derivatives of Ecuación 1 and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are shown in Ecuación 2.\n\\[\n\\begin{aligned}\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} &= -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)  \\\\\n\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)\n\\end{aligned}\n\\tag{2}\\]\nThe derivation of deriving \\(\\hat{\\beta}_0\\) is shown in Ecuación 3.\n\\[\n\\begin{aligned}\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\&\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\&\\Rightarrow n\\hat{\\beta}_0  = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\&\\Rightarrow \\hat{\\beta}_0  = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\&\\Rightarrow \\hat{\\beta}_0  = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\\end{aligned}\n\\tag{3}\\]\nThe derivation of \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived is shown in Ecuación 4.\n\\[\n\\begin{aligned}&\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0  \\\\&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\\\text{(Fill in }\\hat{\\beta}_0\\text{)}&\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\&\\Rightarrow  (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\&\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\&\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big)  = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\ &\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2}\\end{aligned}\n\\tag{4}\\]\nTo write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following:\n\\[\n\\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y)\n\\tag{5}\\]\n\\[\n\\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2\n\\tag{6}\\]\nwhere \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation).\nThus, applying Ecuación 5 and Ecuación 6, we have\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\&= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\&= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\&= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\tag{7}\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into Ecuación 7, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\tag{8}\\]"
  },
  {
    "objectID": "supplemental/log-transformations.html",
    "href": "supplemental/log-transformations.html",
    "title": "Log Transformations in Linear Regression",
    "section": "",
    "text": "Nota\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document provides details about the model interpretation when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model as shown in Ecuación 1.\n\\[\n\\label{orig}\ny = \\beta_0 + \\beta_1 x\n\\tag{1}\\]\nAll results and interpretations can be easily extended to transformations in multiple regression models.\nNote: log refers to the natural logarithm."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-response-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the response variable",
    "text": "Log-transformation on the response variable\nSuppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in Ecuación 2.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 x\n\\tag{2}\\]\nIf we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand.\nIn order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in Ecuación 2 for interpretations and predictions, we will use Ecuación 3 to state our conclusions in terms of \\(y\\).\n\\[\n\\begin{aligned}\n&\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\n\\end{aligned}\n\\tag{3}\\]\nIn order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.\n\nMean, Median, and Log Transformations\nSuppose we have a dataset y that contains the following observations:\n\n\n[1] 3 5 6 7 8\n\n\nIf we log-transform the values of y then calculate the mean and median, we have\n\n\n\n\n\nmean_log_y\nmedian_log_y\n\n\n\n\n1.70503\n1.79176\n\n\n\n\n\nIf we calculate the mean and median of y, then log-transform the mean and median, we have\n\n\n\n\n\nlog_mean\nlog_median\n\n\n\n\n1.75786\n1.79176\n\n\n\n\n\nThis is a simple illustration to show\n\n\\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable\n\\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable\n\n\n\nInterpretaton of model coefficients\nUsing Ecuación 2, the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\).\nSince the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\).\nIntercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is\n\\[\n\\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\}\n\\]\nThus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\)."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the predictor variable",
    "text": "Log-transformation on the predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in #eq-log-x.\n\\[\ny = \\beta_0 + \\beta_1 \\log(x)\n\\tag{4}\\]\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\).\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\).\nSlope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is\n\\[\n\\begin{aligned}\n[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt]\n& = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt]\n& = \\beta_1 \\log(C)\n\\end{aligned}\n\\]\nThus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units.\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "href": "supplemental/log-transformations.html#log-transformation-on-the-the-response-and-predictor-variable",
    "title": "Log Transformations in Linear Regression",
    "section": "Log-transformation on the the response and predictor variable",
    "text": "Log-transformation on the the response and predictor variable\nSuppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in Ecuación 5.\n\\[\n\\log(y) = \\beta_0 + \\beta_1 \\log(x)\n\\tag{5}\\]\nBecause the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail).\nIntercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\),\n\\[\n\\begin{aligned}\n&\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt]\n\\Rightarrow &y = \\exp\\{\\beta_0\\}\n\\end{aligned}\n\\]\nInterpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\).\nSlope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is multiplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is\n\\[\n\\begin{aligned}\n\\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &=\n\\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt]\n& = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1}\n\\end{aligned}\n\\]\nThus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\).\nInterpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html",
    "href": "supplemental/model-selection-criteria.html",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "",
    "text": "Nota\n\n\n\nThe following supplemental notes were created by Dr. Maria Tackett for STA 210. They are provided for students who want to dive deeper into the mathematics behind regression and reflect some of the material covered in STA 211: Mathematics of Regression. Additional supplemental notes will be added throughout the semester.\nThis document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Notation for Multiple Linear Regression for a review."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "href": "supplemental/model-selection-criteria.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)",
    "text": "Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)\nTo understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression.\nLet \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then,\n\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\n\\tag{1}\\]\nWhen we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from Ecuación 1. In Matrix Notation for Multiple Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation.\nA likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using Ecuación 1, we will write the likelihood function for linear regression as\n\\[\nL(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\}\n\\tag{2}\\]\nwhere \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in Ecuación 2, i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is\n\\[\n\\begin{aligned}\n\\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\\n&= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\\n\\end{aligned}\n\\tag{3}\\]\n\nThe maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS\n\\tag{4}\\]\nwhere \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in Ecuación 4 is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial."
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#aic",
    "href": "supplemental/model-selection-criteria.html#aic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "AIC",
    "text": "AIC\nAkaike’s Information Criterion (AIC) is\n\\[\nAIC = -2 \\log L + 2(p+1)\n\\tag{5}\\]\nwhere \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression.\n\\[\n\\begin{aligned}\nAIC &= -2 \\log L + 2(p+1) \\\\\n&= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\\n&= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\\n&= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1)\n\\end{aligned}\n\\tag{6}\\]"
  },
  {
    "objectID": "supplemental/model-selection-criteria.html#bic",
    "href": "supplemental/model-selection-criteria.html#bic",
    "title": "Model Selection Criteria: AIC & BIC",
    "section": "BIC",
    "text": "BIC\n[To be added.]"
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.1.4     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 0.1.4 ──\n\n\n✓ broom        0.7.10         ✓ rsample      0.1.1     \n✓ dials        0.0.10         ✓ tune         0.1.6     \n✓ infer        1.0.1.9000     ✓ workflows    0.2.4     \n✓ modeldata    0.1.1          ✓ workflowsets 0.1.0     \n✓ parsnip      0.1.7          ✓ yardstick    0.0.9     \n✓ recipes      0.2.0          \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 × 9\n   .rownames body_mass_g flipper_length_… .fitted  .resid    .hat .sigma .cooksd\n   &lt;chr&gt;           &lt;int&gt;            &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# … with 332 more rows, and 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "href": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you’ll upload your PDF and them mark the page(s) where each question can be found. It’s OK if a question spans multiple pages, just mark them all. It’s also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "class/class-1.html",
    "href": "class/class-1.html",
    "title": "Clase 1",
    "section": "",
    "text": "Ideas fuerza\n\n\n\n\nNos enfrentamos a un escenario de expansión digital en que las compañias, gobiernos y organizaciones acumulan grandes niveles de información denominado comúnmente como Big Data.\nEl Big Data no es solo cantidad, sino que otra serie de cualidades: volumen, variedad, velocidad, veracidad, valor, vago y vacuo.\nLas Ciencias Sociales Computacionales (CSC) utilizan herramientas de la programación y la estadística para procesar, analizar, modelar y predecir datos conductuales y relacionales de distinta naturaleza.\nEl objetivo de las CSC es robustecer las teorías e hipótesis clásicas de las ciencias sociales, a su vez de aproximar a nuevas preguntas para comprender nuestro vínculo con otros y otras."
  },
  {
    "objectID": "class/class-1.html#clase",
    "href": "class/class-1.html#clase",
    "title": "Clase 1",
    "section": "Clase",
    "text": "Clase\n\n\nPuedes descargar la presentación en PDF dando Click aquí ⏎"
  },
  {
    "objectID": "class/class-1.html#literatura",
    "href": "class/class-1.html#literatura",
    "title": "Clase 1",
    "section": "Literatura",
    "text": "Literatura\n\nSalganik, Matthew J. 2018. Bit by Bit: Social Research in the Digital Age. Princeton, New Jersey: Princeton University Press. Click aquí ⏎\n\nCapítulo 2, pp.12-39\n\nCioffi-Revilla, C. (2014). Introduction to computational social science. London and Hei-delberg: Springer. 2ºEdición. Click aquí ⏎\nLazer, David et al 2009. ‘Computational Social Science’. Science 323(5915):721–23. Click aquí ⏎\n\n\n\nVuelve a la descripción del curso ⏎"
  },
  {
    "objectID": "table_contents.html",
    "href": "table_contents.html",
    "title": "APPDD-23: Aproximación a las políticas públicas desde los datos",
    "section": "",
    "text": "Esta página contiene un resumen de las fechas, temas, el contenido y las tareas del curso. Tenga en cuenta que este cronograma se actualizará en caso de problemas o inconvenientes con todos los cambios documentados aquí.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\nAE\nLab\nHW\nExam\nProject\n\n\n\n1\nWed, Jan 5\nLab 0 - Meet + greet\n\n🖥️\n\n💻\n\n\n\n\n\n\nThu, Jan 6\nWelcome to STA 210!\n📖\n🖥️\n📋\n\n\n\n\n\n\n2\nMon, Jan 10\nLab 1 - Meet the toolkit\n\n🖥️\n\n💻\n\n\n\n\n\n\nTue, Jan 11\nSimple linear regression (SLR)\n📖\n🖥️\n\n\n\n\n\n\n\n\nThu, Jan 13\nSLR: Model fitting in R with tidymodels\n\n🖥️\n📋\n\n\n\n\n\n\n\nFri, Jan 14\nReleased: HW 5\n\n\n\n\n✍️\n\n\n\n\n\n\nDue: Lab 1\n\n\n\n\n💻 🗝️\n\n\n\n\n\n\n3\nMon, Jan 17\nMartin Luther King, Jr. Day\n\n\n\n\n\n\n\n\n\n\nTue, Jan 18\nSLR: Prediction + model evaluation\n📖\n🖥️\n📋\n\n\n\n\n\n\n\nThu, Jan 20\nSLR: Simulation-based inference\n\n🖥️\n\n\n\n\n\n\n\n\nFri, Jan 21\nReleased: HW 1\n\n\n\n\n✍️\n\n\n\n\n4\nMon, Jan 24\nLab 2 - College scorecard\n\n🖥️\n\n💻\n\n\n\n\n\n\nTue, Jan 25\nSLR: Mathematical models for inference\n📖\n🖥️\n\n\n\n\n\n\n\n\nThu, Jan 27\nSLR: Model diagnostics\n\n🖥️\n📋\n\n\n\n\n\n\n\nFri, Jan 28\nDue: HW 1 + Lab 2\n\n\n\n\n💻 🗝️\n\n\n✍️ 🗝️\n\n\n\n\n\n5\nMon, Jan 31\nLab 3 - Coffee ratings\n\n🖥️\n\n💻\n\n\n\n\n\n\nTue, Feb 1\nMultiple linear regression (MLR)\n📖\n🖥️\n\n\n\n\n\n\n\n\nThu, Feb 3\nExam 1 review\n\n🖥️\n📋\n\n\n\n\n\n\n\nFri, Feb 4\nReleased: Exam 1\n\n\n\n\n\n✅\n\n\n\n\n\nDue: Lab 3\n\n\n\n\n💻 🗝️\n\n\n\n\n\n\n6\nMon, Feb 7\nNo lab: Work on Exam 1\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam 1\n\n\n\n\n\n✅\n\n\n\n\nTue, Feb 8\nMLR: Types of predictors\n📖\n🖥️\n\n\n\n\n\n\n\n\nThu, Feb 10\nMLR: Model comparison\n\n🖥️\n\n\n\n\n\n\n\n\nFri, Feb 11\nReleased: HW 2\n\n\n\n\n✍️\n\n\n\n\n7\nMon, Feb 14\nLab: Project topic ideas\n\n\n\n\n\n\n\n📂\n\n\n\nTue, Feb 15\nMLR: Feature engineering\n📖\n🖥️\n📋\n\n\n\n\n\n\n\nThu, Feb 17\nMLR: Feature engineering\n\n🖥️\n\n\n\n\n\n\n\n\nFri, Feb 18\nDue: HW 2 + Project topic ideas\n\n\n\n\n\n✍️ 🗝️\n\n\n📂\n\n\n8\nMon, Feb 21\nLab 4: The Office\n\n\n\n💻\n\n\n\n\n\n\nTue, Feb 22\nMLR: Cross validation\n📖\n🖥️\n📋\n\n\n\n\n\n\n\nThu, Feb 24\nExam 2 review\n\n🖥️\n📋\n\n\n\n\n\n\n\nFri, Feb 25\nReleased: Exam 2\n\n\n\n\n\n✅\n\n\n\n\n\nDue: Lab 4\n\n\n\n\n💻 🗝️\n\n\n\n\n\n\n9\nMon, Feb 28\nNo lab: Work on Exam 2\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam 2\n\n\n\n\n\n✅\n\n\n\n\nTue, Mar 1\nMLR: Inference\n📖\n🖥️\n📋\n\n\n\n\n\n\n\nThu, Mar 3\nMLR: Inference conditions + multicollinearity\n\n🖥️\n\n\n\n\n\n\n\n\nMon, Mar 7\nSpring break\n\n\n\n\n\n\n\n\n\n\nTue, Mar 8\nSpring break\n\n\n\n\n\n\n\n\n\n\nThu, Mar 10\nSpring break\n\n\n\n\n\n\n\n\n\n10\nMon, Mar 14\nLab: Work on project proposals\n\n\n\n\n\n\n\n📂\n\n\n\nTue, Mar 15\nLogistic regression (LR)\n📖\n🖥️\n📋\n\n\n\n\n\n\n\nThu, Mar 17\nProbabilities, odds, and odds ratios\n\n🖥️\n\n\n\n\n\n\n\n\nFri, Mar 18\nDue: Project proposals\n\n\n\n\n\n\n📂\n\n\n\n\nReleased: HW 3\n\n\n\n\n✍️\n\n\n\n\n11\nMon, Mar 21\nLab 5: General Social Survey\n\n\n\n💻\n\n\n\n\n\n\nTue, Mar 22\nLR: Prediction / classification\n📖\n🖥️\n\n\n\n\n\n\n\n\nThu, Mar 24\nLR: Model comparison\n\n🖥️\n📋\n\n\n\n\n\n\n\nFri, Mar 25\nDue: HW 3 + Lab 5\n\n\n\n\n💻 🗝️\n\n\n✍️ 🗝️\n\n\n\n\n\n12\nMon, Mar 28\nLab: Work on project drafts\n\n\n\n\n\n\n\n📂\n\n\n\nTue, Mar 29\nLR: Inference + conditions\n📖\n🖥️\n\n\n\n\n\n\n\n\nThu, Mar 31\nMultinomial Logistic Regression (MultiLR)\n\n🖥️\n\n\n\n\n\n\n\n\nFri, Apr 1\nReleased: HW 4\n\n\n\n\n✍️\n\n\n\n\n13\nMon, Apr 4\nLab 6: Why Many Americans Don’t Vote\n\n\n\n💻\n\n\n\n\n\n\nTue, Apr 5\nMultiLR: Prediction + inferential models\n📖\n🖥️\n\n\n\n\n\n\n\n\nThu, Apr 7\nMultiLR: Predictive models\n\n🖥️\n📋\n\n\n\n\n\n\n\nFri, Apr 8\nDue: Lab 6\n\n\n\n\n💻 🗝️\n\n\n\n\n\n\n\nSun, Apr 10\nDue: Project drafts\n\n\n\n\n\n\n📂\n\n\n14\nMon, Apr 11\nLab: Project peer review of drafts\n\n\n\n\n\n\n\n📂\n\n\n\nTue, Apr 12\nMultiLR: Predictive models (cont.)\n📖\n🖥️\n\n\n\n\n\n\n\n\n\nDue: HW 4\n\n\n\n\n\n✍️ 🗝️\n\n\n\n\n\n\nThu, Apr 14\nExam 3 review\n\n🖥️\n📋\n\n\n\n\n\n\n\n\nDue: Peer review\n\n\n\n\n\n\n📂\n\n\n\nFri, Apr 15\nReleased: Exam 3\n\n\n\n\n\n✅\n\n\n\n15\nMon, Apr 18\nNo lab: Work on Exam 3\n\n\n\n\n\n\n\n\n\n\n\nDue: Exam 3\n\n\n\n\n\n✅\n\n\n\n\nTue, Apr 19\nWrap-up\n📖\n🖥️\n📋\n\n\n\n\n\n\n\n\nDue: HW 5\n\n\n\n\n✍️\n\n\n\n\n16\nMon, Apr 25\nDue: Project write-up\n\n\n\n\n\n\n\n📂\n\n\n\nThu, Apr 28\nDue: Video presentation + repo\n\n\n\n\n\n\n📂\n\n\n\nSat, Apr 30\nDue: Video comments\n\n\n\n\n\n\n\n📂"
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Fri, Feb 18\nProposal due Fri, Mar 18\nDraft report due Fri, Apr 8\nPeer review due Fri, Apr 15\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30"
  },
  {
    "objectID": "project-description.html#timeline",
    "href": "project-description.html#timeline",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Fri, Feb 18\nProposal due Fri, Mar 18\nDraft report due Fri, Apr 8\nPeer review due Fri, Apr 15\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30"
  },
  {
    "objectID": "project-description.html#introduction",
    "href": "project-description.html#introduction",
    "title": "Project description",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group’s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio, and all components of the project must be reproducible (with the exception of the presentation).\n\nLogistics\nYou will work on the project with your lab groups.\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis\nA GitHub repository corresponding to your report\nSlides + a video presentation\nFormal peer review on another team’s project"
  },
  {
    "objectID": "project-description.html#topic-ideas",
    "href": "project-description.html#topic-ideas",
    "title": "Project description",
    "section": "Topic ideas",
    "text": "Topic ideas\nIdentify 2-3 data sets you’re interested in potentially using for the final project. If you’re unsure where to find data, you can use the list of potential data sources in the Tips + Resources section as a starting point. It may also help to think of topics you’re interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help you be successful in the project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by the teaching team.\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as “name”, “social security number”, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\nPlease ask a member of the teaching team if you’re unsure whether your data set meets the criteria.\nFor each data set, include the following:\n\nIntroduction and data\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data)\nDescribe the observations and the general characteristics being measured in the data\n\n\n\nResearch question\n\nDescribe a research question you’re interested in answering using this data.\n\n\n\nGlimpse of data\n\nUse the glimpse function to provide an overview of each data set\n\nSubmit the PDF of the topic ideas to Gradescope. Mark all pages associated with each data set."
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Project description",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is to help you think about your analysis strategy early.\nInclude the following in the proposal:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating\nthe motivation for your research question (citing any relevant literature)\nthe general research question you wish to explore\nyour hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data set you wish to explore. This includes\n\ndescription of the observations in the data set,\ndescription of how the data was originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the response variable.\nVisualization and summary statistics for the response variable.\nList of variables that will be considered as predictors\nRegression model technique (multiple linear regression and logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of your project repo, in the data folder. Link to this file from your proposal writeup.\n\n\nSubmission\nPush all of your final changes to the GitHub repo, and submit the PDF of your proposal to Gradescope.\n\n\nProposal grading\n\n\n\nTotal\n10 pts\n\n\n\n\nIntroduction\n3 pts\n\n\nData description\n2 pts\n\n\nAnalysis plan\n4 pts\n\n\nData dictionary\n1 pts\n\n\n\nEach component will be graded as follows:\n\nMeets expectations (full credit): All required elements are completed and are accurate. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nClose to expectations (half credit): There are some elements missing and/or inaccurate. There are some issues with formatting.\nDoes not meet expectations (no credit): Major elements missing. Work is not neatly formatted and would not be presentable in a professional setting."
  },
  {
    "objectID": "project-description.html#draft-report",
    "href": "project-description.html#draft-report",
    "title": "Project description",
    "section": "Draft report",
    "text": "Draft report\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model."
  },
  {
    "objectID": "project-description.html#peer-review",
    "href": "project-description.html#peer-review",
    "title": "Project description",
    "section": "Peer review",
    "text": "Peer review\nCritically reviewing others’ work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teams’s projects to review. Each team should push their draft to their GitHub repo by the due date. One lab in the following week will be dedicated to the peer review, and all reviews will be due by the end of that lab session.\nDuring the peer review process, you will be provided read-only access to your partner teams’ GitHub repos. Provide your review in the form of GitHub issues to your partner team’s GitHub repo using the issue template provided. The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team’s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\n\nPairings\n\nSection 1 - M 1:45PM - 3:00PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\nchaa_chaa_chaa\nyay_stats\nstat_over_flow\n\n\ndekk\nchaa_chaa_chaa\nyay_stats\n\n\neight\ndekk\nchaa_chaa_chaa\n\n\nhousecats\neight\ndekk\n\n\nkrafthouse\nhousecats\neight\n\n\nrrawr\nkrafthouse\nhousecats\n\n\nstat_over_flow\nrrawr\nkrafthouse\n\n\nyay_stats\nstat_over_flow\nrrawr\n\n\n\n\n\nSection 2 - M 3:30PM - 4:45PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\na_plus_plus_plus\nwe_r\ntina\n\n\npredictors\na_plus_plus_plus\nwe_r\n\n\nsixers\npredictors\na_plus_plus_plus\n\n\nsoy_nuggets\nsixers\npredictors\n\n\ntina\nsoy_nuggets\nsixers\n\n\nwe_r\ntina\nsoy_nuggets\n\n\n\n\n\nSection 3 - M 5:15PM - 6:30PM\n\n\n\n\n\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\ndown_to_earth_goats\nthe_three_musketeers\nteam_five\n\n\nginger_and_stats\ndown_to_earth_goats\nthe_three_musketeers\n\n\npineapple_wedge_and_diced_papaya\nginger_and_stats\ndown_to_earth_goats\n\n\nstatchelorettes\npineapple_wedge_and_diced_papaya\nginger_and_stats\n\n\nstatisix\nstatchelorettes\npineapple_wedge_and_diced_papaya\n\n\nstats_squad\nstatisix\nstatchelorettes\n\n\nteam_five\nstats_squad\nstatisix\n\n\nthe_three_musketeers\nteam_five\nstats_squad\n\n\n\n\n\n\nProcess and questions\nSpend ~30 mins to review each team’s project.\n\nFind your team name on the Reviewer 1 and Reviewer 2 columns.\nFor each of the columns, find the name of the team to review in the Team being reviewed column. You should already have access to this team’s repo.\nOpen the repo of the team you’re reviewing, read their project draft, and browser around the rest of their repo.\nThen, go to the Issues tab in that repo, click on New issue, and click on Get started for the Peer review issue. Fill out this issue, answering the following questions:\n\nPeer review by: [NAME OF TEAM DOING THE REVIEW]\nNames of team members that participated in this review: [FULL NAMES OF TEAM MEMBERS DOING THE REVIEW]\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nIs there anything that is unclear from the proposal?\nProvide constructive feedback on how the team might be able to improve their project. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?"
  },
  {
    "objectID": "project-description.html#written-report",
    "href": "project-description.html#written-report",
    "title": "Project description",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\nBefore you finalize your write up, make sure the printing of code chunks is off with the option echo = FALSE.\nYou will submit the PDF of your final report on Gradescope.\nThe PDF you submit must match the files in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\nClick here for a PDF of the written report rubric.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model’s predictive power is thoroughly assessed.\n\n\n\nDiscussion + Conclusion\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages."
  },
  {
    "objectID": "project-description.html#video-presentation-slides",
    "href": "project-description.html#video-presentation-slides",
    "title": "Project description",
    "section": "Video presentation + slides",
    "text": "Video presentation + slides\n\nSlides\nIn addition to the written report, your team will also create presentation slides and record a video presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nFor submission, convert these slides to a .pdf document, and submit the PDF of the slides on Gradescope.\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\nVideo presentation\nFor the video presentation, you can speak over your slide deck, similar to the lecture content videos. The video presentation must be no longer than 8 minutes. It is fine if the video is shorter than 8 minutes, but it cannot exceed 8 minutes. You may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire, then embed the video in an new discussion post on Conversations.\n\nTo upload your video to Warpwire:\n\nClick the Warpwire tab in the course Sakai site.\nClick the “+” and select “Upload files”.\nLocate the video on your computer and click to upload.\nOnce you’ve uploaded the video to Warpwire, click to share the video and copy the video’s URL. You will need this when you post the video in the discussion forum.\n\n\n\nTo post the video to the discussion forum\n\nClick the Presentations tab in the course Sakai site.\nClick the Presentations topic.\nClick “Start a new conversation”.\nMake the title “Your Team Name: Project Title”. For example, “Teaching Team: Our Awesome Presentation”.\nClick the Warpwire icon (between the table and shopping cart icons).\nSelect your video, then click “Insert 1 item.” This will embed your video in the conversation.\nUnder the video, paste the URL to your video.\nYou’re done!"
  },
  {
    "objectID": "project-description.html#presentation-comments",
    "href": "project-description.html#presentation-comments",
    "title": "Project description",
    "section": "Presentation comments",
    "text": "Presentation comments\nEach student will be assigned 2 presentations to watch. Your viewing assignments will be posted later in the semester.\nWatch the group’s video, then click “Reply” to post a question for the group. You may not post a question that’s already been asked on the discussion thread. Additionally, the question should be (i) substantive (i.e. it shouldn’t be “Why did you use a bar plot instead of a pie chart”?), (ii) demonstrate your understanding of the content from the course, and (iii) relevant to that group’s specific presentation, i.e demonstrating that you’ve watched the presentation.\nThis portion of the project will be assessed individually.\n\nPairings\nFind your team name in the first column, watch videos from teams in the second column and leave comments.\n\n\n\n\n\n\n\n\nReviewer\nFirst video to review\nSecond video to review\n\n\n\n\nGinger and Stats\nEight\nWe R\n\n\nKrafthouse\nGinger and Stats\nEight\n\n\nSoy Nuggets\nKrafthouse\nGinger and Stats\n\n\nDown To Earth Goats\nSoy Nuggets\nKrafthouse\n\n\nA+++\nDown To Earth Goats\nSoy Nuggets\n\n\nTeam Five\nA+++\nDown To Earth Goats\n\n\nRrawr\nTeam Five\nA+++\n\n\nHousecats\nRrawr\nTeam Five\n\n\nDekk\nHousecats\nRrawr\n\n\nStat OverFlow\nDekk\nHousecats\n\n\nThe Three Musketeers\nStat OverFlow\nDekk\n\n\nPredictors\nThe Three Musketeers\nStat OverFlow\n\n\nStats Squad\nPredictors\nThe Three Musketeers\n\n\nStatisix\nStats Squad\nPredictors\n\n\nSixers\nStatisix\nStats Squad\n\n\nYay Stats\nSixers\nStatisix\n\n\nTINA\nYay Stats\nSixers\n\n\nStatchelorettes\nTINA\nYay Stats\n\n\nPineapple Wedge and Diced Papaya\nStatchelorettes\nTINA\n\n\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\nStatchelorettes\n\n\nWe R\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\n\n\nEight\nWe R\nChaa Chaa Chaa"
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project description",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/previous-work: Folder that contains the topic-ideas and project-proposal files.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-description.html#peer-teamwork-evaluation",
    "href": "project-description.html#peer-teamwork-evaluation",
    "title": "Project description",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.\nIf you have concerns with the teamwork and/or contribution from any team members, please email me by the project video deadline. You only need to email me if you have concerns. Otherwise, I will assume everyone on the team equally contributed and will receive full credit for the teamwork portion of the grade."
  },
  {
    "objectID": "project-description.html#overall-grading",
    "href": "project-description.html#overall-grading",
    "title": "Project description",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n5 pts\n\n\nProject proposal\n10 pts\n\n\nPeer review\n10 pts\n\n\nWritten report\n40 pts\n\n\nSlides + video presentation\n20 pts\n\n\nReproducibility + organization\n5 pts\n\n\nVideo comments\n5 pts\n\n\nPeer teamwork evaluation\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\nLate work policy\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "course-contact.html",
    "href": "course-contact.html",
    "title": "Contacto con el profesor",
    "section": "",
    "text": "Todas los pdfs de clases, los laboratorios y actualizaciones serán subidos en esta plataforma y en el moodle. Los videos de las clases no se pueden subir a esta plataforma, pero estarán disponible en moodle.\nTendremos dos vías de comunicación:\n\nSlack del curso: actuaremos en una lógica de foro para esto utilizaremos una plataforma llamada slack donde pueden preguntar y las respuestas estarán disponibles para otros. Pueden acceder al slack creandose una cuenta y buscando el canal: C057ZU1S4RJ, también pueden dar click aquí.\nContacto vía correo: pueden hacer dudas o solicitar un horario de atención a jdconejeros@uc.cl."
  }
]